{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage Data Stage 2, Parsed Data + WE to Pytorch Geometric data batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "## util\n",
    "import os\n",
    "import logging\n",
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm_notebook as tqdmnb\n",
    "from tqdm import tqdm as tqdm\n",
    "import pickle\n",
    "import json \n",
    "import jsonlines as jsonl\n",
    "## graph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "## nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from torch_geometric.data.data import Data\n",
    "from torch_geometric.data import DataLoader\n",
    "## model\n",
    "import stanza\n",
    "from stanza.models.common.doc import Document\n",
    "\n",
    "## self\n",
    "import config\n",
    "from config import nli_config \n",
    "import utils\n",
    "from model import *\n",
    "import data\n",
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_data = utils.load_glove_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove, words, word2idx, idx = glove_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANLI_example = \"Kwon So-hyun (born August 30, 1994), is a South Korean singer and actress. She was known as a member of the South Korean girl group 4Minute, under Cube Entertainment. She is also a former member of the South Korean girl group, Orange. Before June 15, 2016 (end of her contract with Cube), Sohyun left 4Minute and the record label along with members Nam Ji-hyun, Heo Ga-yoon, and Jeon Ji-yoon.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = utils.text2graph(ANLI_example, nlp)\n",
    "utils.draw(g2, node_size=100, font_size=5, save_img_file=\"anli_demo_graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove[word2idx[\"good\"]]@glove[word2idx[\"nice\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove[word2idx[\"good\"]]@glove[word2idx[\"bad\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_jdata = data.load_jdata(data_file=config.PDEV_MA_FILE)\n",
    "test_jdata = data.load_jdata(data_file=config.PDEV_MMA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = data.GraphDataset(config.PTRAIN_FILE, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data_set = data.GraphDataset(config.PDEV_MA_FILE, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_set = data.GraphDataset(config.PDEV_MMA_FILE, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dev_data_set, batch_size=4, follow_batch=['x_p', 'x_h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch.x_p_batch, batch.label, batch.pid, sep='\\n')\n",
    "print(batch.edge_index_p, batch.edge_index_h, batch.label, batch.pid, sep='\\n')\n",
    "print(batch.x_p, batch.x_h, sep='\\n')\n",
    "print(batch.x_p_batch, batch.x_h_batch, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utils.token2sent(batch.x_h, words))\n",
    "print(utils.token2sent(batch.x_p, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SynNLI_Model(pretrained_embedding_tensor=glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = train.SynNLI_Trainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, stat = trainer.train(train_set=dev_data_set[:5],\n",
    "              dev_set=dev_data_set[:5],\n",
    "             word2idx=word2idx,\n",
    "             pretrained_word_vectors=glove,\n",
    "             model_file_path=config.SAVE_MODEL_FOLDER,\n",
    "             batch_size=config.BATCH_SIZE,\n",
    "             follow_batch=config.follow_batch)\n",
    "trained_model = trained_model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = data.GraphData(dev_jdata[0], word2idx)\n",
    "b = data.GraphData(dev_jdata[1], word2idx)\n",
    "c = data.GraphData(dev_jdata[1], word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(c.label.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader = DataLoader([a,b,c], batch_size=3, follow_batch=[\"x_p\", \"x_h\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testb = next(iter(dev_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testb.label)\n",
    "print(testb.label.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testb.label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader as DL\n",
    "class test_data():\n",
    "    def __init__(self,a):\n",
    "        self.data = torch.tensor([a,a,a])\n",
    "    \n",
    "a = test_data(1).data\n",
    "b = test_data(2).data\n",
    "c = test_data(3).data\n",
    "dl = DL([a,b,c], batch_size=3)\n",
    "testb = next(iter(dl))\n",
    "print(testb.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SynNLI_Model(pretrained_embedding_tensor=glove)\n",
    "trainer = train.SynNLI_Trainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, stat = trainer.train(train_set=train_set,\n",
    "              dev_set=dev_data_set,\n",
    "             word2idx=word2idx,\n",
    "                                    \n",
    "                                    \n",
    "             pretrained_word_vectors=glove,\n",
    "             model_file_path=config.SAVE_MODEL_FOLDER,\n",
    "             batch_size=config.BATCH_SIZE,\n",
    "             follow_batch=config.follow_batch)\n",
    "trained_model = trained_model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SynNLI_Model(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation): ReLU(inplace=True)\n",
       "  (embedding): Embedding(1917496, 300)\n",
       "  (encoder): GraphEncoder(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (activation): ReLU(inplace=True)\n",
       "    (conv): GATConv(300, 100, heads=3)\n",
       "  )\n",
       "  (cross_att): CrossAttentionLayer(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (activation): ReLU(inplace=True)\n",
       "    (Wq): Linear(in_features=300, out_features=300, bias=False)\n",
       "    (Wk): Linear(in_features=300, out_features=300, bias=False)\n",
       "    (Wv): Linear(in_features=300, out_features=300, bias=False)\n",
       "  )\n",
       "  (local_cmp): Sequential(\n",
       "    (0): Linear(in_features=1200, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=600, out_features=300, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=300, out_features=3, bias=True)\n",
       "  )\n",
       "  (criterion): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANLI_example = \"Kwon So-hyun (born August 30, 1994), is a South Korean singer and actress. She was known as a member of the South Korean girl group 4Minute, under Cube Entertainment. She is also a former member of the South Korean girl group, Orange. Before June 15, 2016 (end of her contract with Cube), Sohyun left 4Minute and the record label along with members Nam Ji-hyun, Heo Ga-yoon, and Jeon Ji-yoon.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANLI2 = \"Don Wayne Reno (born February 8, 1963 in Roanoke, Virginia) is a bluegrass musician and banjo player, and also an ordained minister. He is a son of famed bluegrass musician Don Reno. Reno was for several years a mainstay of Hayseed Dixie with his brother Dale Reno as the mandolinist. He currently works with his brother and Mitch Harrell in the band Reno and Harrell.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probing Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nANLI2 =\\n\"Don Wayne Reno (born February 8, 1963 in Roanoke, Virginia) is a bluegrass musician and banjo player, and also an ordained minister.\\nHe is a son of famed bluegrass musician Don Reno.\\nReno was for several years a mainstay of Hayseed Dixie with his brother Dale Reno as the mandolinist.\\nHe currently works with his brother and Mitch Harrell in the band Reno and Harrell.\"\\n'"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data_list = [ {\n",
    "    config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(\"He is a cat with furries\").to_dict(),\n",
    "    config.hf : nlp(\"He is a cat\").to_dict(),\n",
    "    config.idf : \"0001e\"\n",
    "},\n",
    "{\n",
    "    config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(\"He is a cat\").to_dict(),\n",
    "    config.hf : nlp(\"He is not a cat\").to_dict(),\n",
    "    config.idf : \"0001e\"\n",
    "},\n",
    "{\n",
    "    config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(\"He is a good cat\").to_dict(),\n",
    "    config.hf : nlp(\"He is good\").to_dict(),\n",
    "    config.idf : \"0001e\"\n",
    "},\n",
    "{\n",
    "    config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(\"He is a good cat\").to_dict(),\n",
    "    config.hf : nlp(\"He is a bad cat\").to_dict(),\n",
    "    config.idf : \"0001e\"\n",
    "},\n",
    "{# wrong\n",
    "    config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(\"He is a good cat\").to_dict(),\n",
    "    config.hf : nlp(\"He is not a bad cat\").to_dict(),\n",
    "    config.idf : \"0001e\"\n",
    "}, \n",
    "{# catch neutral in ANLI!\n",
    "\n",
    "    config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(\"Don Wayne Reno is a musician\").to_dict(),\n",
    "    config.hf : nlp(ANLI2).to_dict(),\n",
    "    config.idf : \"0001e\"\n",
    "}, \n",
    "{# catch entail in ANLI!\n",
    "\n",
    "    config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(ANLI2).to_dict(),\n",
    "    config.hf : nlp(\"Don Wayne Reno is a musician\").to_dict(),\n",
    "    config.idf : \"0001e\"\n",
    "}, \n",
    "{# wrong, basketball\n",
    "\n",
    "    config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(ANLI2).to_dict(),\n",
    "    config.hf : nlp(\"Don Wayne Reno is a basketball player\").to_dict(),\n",
    "    config.idf : \"0001e\"\n",
    "}, \n",
    "{# vcan catch a an no, but cannot catch other adj that can harm maning of words\n",
    "\n",
    "    config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(ANLI2).to_dict(),\n",
    "    config.hf : nlp(\"Don Wayne Reno has no brother\").to_dict(), # test a, no , aged, handsome\n",
    "    config.idf : \"0001e\"\n",
    "}, \n",
    "    {# vcan catch a an no, but cannot catch other adj that can harm maning of words\n",
    "\n",
    "    config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(\"He did it from top to bottom\").to_dict(),\n",
    "    config.hf : nlp(\"He did it from bottom to top\").to_dict(), # test a, no , aged, handsome\n",
    "    config.idf : \"0001e\"\n",
    "}, \n",
    "]\n",
    "\"\"\"\n",
    "ANLI2 =\n",
    "\"Don Wayne Reno (born February 8, 1963 in Roanoke, Virginia) is a bluegrass musician and banjo player, and also an ordained minister.\n",
    "He is a son of famed bluegrass musician Don Reno.\n",
    "Reno was for several years a mainstay of Hayseed Dixie with his brother Dale Reno as the mandolinist.\n",
    "He currently works with his brother and Mitch Harrell in the band Reno and Harrell.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = DataLoader([data.GraphData(my_data, word2idx) for my_data in my_data_list], batch_size=32, follow_batch=config.follow_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 2, 0, 0, 1, 2, 2, 0, 2])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model._predict(next(iter(tl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['contradiction', 'neutral', 'entailment'])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn.LSTM(input_size=d, hidden_size=d*2, num_layers=2, bidirectional=True, batch_first=True, dropout=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After This, Code Will Be Migrated to New Coding Style, And Repo will be restart\n",
    "- the following lines are trainning code for model that can work before stopping develop the src-gnn folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "model22 = SynNLI_Model(nli_config=config.nli_config, pretrained_embedding_tensor=glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer device is cuda\n"
     ]
    }
   ],
   "source": [
    "trainer22 = train.SynNLI_Trainer(model22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no log_stream specified, logger printing to default path/work/2020-IIS-NLU-internship/MNLI/param/SynNLIv0.1_glove_GAT3_test_before_migration/train_log.txt\n",
      "no stat_stream specified, pure statistics printing to default path/work/2020-IIS-NLU-internship/MNLI/param/SynNLIv0.1_glove_GAT3_test_before_migration/stat.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.71it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 28.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 34.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 2, 1]\n",
      "[1, 0, 2, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.74it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.53it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 36.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 1, 2, 2]\n",
      "[1, 0, 2, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 15.79it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.69it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 35.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 1, 2, 2]\n",
      "[1, 0, 2, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.41it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.99it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 35.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 1, 2]\n",
      "[1, 0, 2, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 30.82it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 35.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 2, 1]\n",
      "[1, 0, 2, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "trained_model22, stat = trainer22.train(train_set=train_set[:5],\n",
    "              dev_set=dev_data_set[:5],\n",
    "             word2idx=word2idx,\n",
    "             pretrained_word_vectors=glove,\n",
    "             model_file_path=config.SAVE_MODEL_FOLDER,\n",
    "             batch_size=config.BATCH_SIZE,\n",
    "             follow_batch=config.follow_batch)\n",
    "trained_model22 = trained_model22.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop of development in this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
