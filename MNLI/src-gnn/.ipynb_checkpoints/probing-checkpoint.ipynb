{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "- direct execution should do\n",
    "- load trained model for \n",
    "- load glove embedding + Stanza NLP Pipeline\n",
    "- test with my_data_list\n",
    "    - you can modify data in here\n",
    "- 0, 1, 2 is contradiction, neutral, entail, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "import config\n",
    "import data\n",
    "import utils\n",
    "\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "from torch_geometric.data.data import Data\n",
    "from torch_geometric.data import DataLoader\n",
    "## model\n",
    "import stanza\n",
    "from stanza.models.common.doc import Document\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn import metrics\n",
    "from random import sample\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/work/2020-IIS-NLU-internship/MNLI/param/SynNLIv0.1_glove_GAT3')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = config.PARAM_PATH / \"SynNLIv0.1_glove_GAT3\"\n",
    "PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM = PATH / \"model_epoch4_precision:0.611_recall:0.618_f1:0.612_acc:0.612.m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nli_config_dict_probe = deepcopy(config.nli_config_dict)\n",
    "nli_config_dict_probe[\"embedding\"] = None\n",
    "nli_config_dict_probe = config.Model_Config(nli_config_dict_probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SynNLI_Model(nli_config=nli_config_dict_probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(PARAM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Stanza nlp and GLOVE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-10 16:58:11 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2020-08-10 16:58:11 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "| lemma     | ewt     |\n",
      "| depparse  | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-08-10 16:58:11 INFO: Use device: gpu\n",
      "2020-08-10 16:58:11 INFO: Loading: tokenize\n",
      "2020-08-10 16:58:11 INFO: Loading: pos\n",
      "2020-08-10 16:58:13 INFO: Loading: lemma\n",
      "2020-08-10 16:58:13 INFO: Loading: depparse\n",
      "2020-08-10 16:58:15 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1917494/1917494 [02:27<00:00, 13016.51it/s]\n",
      "/opt/conda/lib/python3.7/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "glove_data = utils.load_glove_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove, words, word2idx, idx = glove_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANLI2 =\"\"\"\n",
    "\"Don Wayne Reno (born February 8, 1963 in Roanoke, Virginia) is a bluegrass musician and banjo player, and also an ordained minister.\n",
    "He is a son of famed bluegrass musician Don Reno.\n",
    "Reno was for several years a mainstay of Hayseed Dixie with his brother Dale Reno as the mandolinist.\n",
    "He currently works with his brother and Mitch Harrell in the band Reno and Harrell.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_list = [ {\n",
    "    config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(\"He is a cat with furries\").to_dict(),\n",
    "    config.hf : nlp(\"He is a cat\").to_dict(),\n",
    "    config.idf : \"0001e\"\n",
    "},\n",
    "{\n",
    "    config.lf : config.label_to_id[\"contradiction\"],\n",
    "    config.pf : nlp(\"He is a cat\").to_dict(),\n",
    "    config.hf : nlp(\"He is not a cat\").to_dict(),\n",
    "    config.idf : \"0001e\"\n",
    "},\n",
    "{\n",
    "    config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(\"He is a good cat\").to_dict(),\n",
    "    config.hf : nlp(\"He is good\").to_dict(),\n",
    "    config.idf : \"0001e\"\n",
    "},\n",
    "{\n",
    "    config.lf : config.label_to_id[\"contradiction\"],\n",
    "    config.pf : nlp(\"He is a good cat\").to_dict(),\n",
    "    config.hf : nlp(\"He is a bad cat\").to_dict(),\n",
    "    config.idf : \"0001e\"\n",
    "},\n",
    "{# wrong\n",
    "    config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(\"He is a good cat\").to_dict(),\n",
    "    config.hf : nlp(\"He is not a bad cat\").to_dict(),\n",
    "    config.idf : \"0001e\"\n",
    "}, \n",
    "{# catch neutral in ANLI!\n",
    "\n",
    "    config.lf : config.label_to_id[\"neutral\"],\n",
    "    config.pf : nlp(\"Don Wayne Reno is a musician\").to_dict(),\n",
    "    config.hf : nlp(ANLI2).to_dict(),\n",
    "    config.idf : \"0001e\"\n",
    "}, \n",
    "{# catch entail in ANLI!\n",
    "\n",
    "    config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(ANLI2).to_dict(),\n",
    "    config.hf : nlp(\"Don Wayne Reno is a musician\").to_dict(),\n",
    "    config.idf : \"0001e\"\n",
    "}, \n",
    "{# wrong, basketball\n",
    "\n",
    "    config.lf : config.label_to_id[\"contradiction\"],\n",
    "    config.pf : nlp(ANLI2).to_dict(),\n",
    "    config.hf : nlp(\"Don Wayne Reno is a basketball player\").to_dict(),\n",
    "    config.idf : \"0001e\"\n",
    "}, \n",
    "{# vcan catch a an no, but cannot catch other adj that can harm maning of words\n",
    "\n",
    "    config.lf : config.label_to_id[\"contradiction\"],\n",
    "    config.pf : nlp(ANLI2).to_dict(),\n",
    "    config.hf : nlp(\"Don Wayne Reno has no brother\").to_dict(), # test a, no , aged, handsome\n",
    "    config.idf : \"0001e\"\n",
    "}, \n",
    "    {# vcan catch a an no, but cannot catch other adj that can harm maning of words\n",
    "\n",
    "    config.lf : config.label_to_id[\"contradiction\"],\n",
    "    config.pf : nlp(\"He did it from top to bottom\").to_dict(),\n",
    "    config.hf : nlp(\"He did it from bottom to top\").to_dict(), # test a, no , aged, handsome\n",
    "    config.idf : \"0001e\"\n",
    "}, \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = DataLoader([data.GraphData(my_data, word2idx) for my_data in my_data_list], batch_size=32, follow_batch=config.follow_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 0, 2, 0, 0, 1, 2, 2, 0, 2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._predict(next(iter(tl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['contradiction', 'neutral', 'entailment'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.id_to_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probing Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probe(model=model, data_list=None):\n",
    "    labels = [\"contradiction\", \"neutral\", \"entailment\"]\n",
    "    loader = DataLoader([data.GraphData(my_data, word2idx) for my_data in data_list], batch_size=32, follow_batch=config.follow_batch)\n",
    "    print([labels[data[config.lf]] for data in data_list])\n",
    "    print([labels[idx]for idx in model._predict(next(iter(loader)))])\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# negation probing\n",
    "- single negation\n",
    "    - simple\n",
    "    - long\n",
    "    - long and change position (actually trivial for graph?)\n",
    "- double negation\n",
    "    - simple \n",
    "    - long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['contradiction', 'contradiction', 'contradiction', 'entailment']\n",
      "['contradiction', 'contradiction', 'contradiction', 'entailment']\n"
     ]
    }
   ],
   "source": [
    "data_neg = [\n",
    "    # simple negation\n",
    "    {config.lf : config.label_to_id[\"contradiction\"],\n",
    "    config.pf : nlp(\"Allen likes to eat pizza.\").to_dict(),\n",
    "    config.hf : nlp(\"Allen does not like to eat pizza\").to_dict(), # test a, no , aged, handsome\n",
    "    config.idf : \"0001e\"},\n",
    "    # long simple negation\n",
    "    {config.lf : config.label_to_id[\"contradiction\"],\n",
    "    config.pf : nlp(\"Tim likes to eat pizza. Tim likes to eat pizza. Tim likes to eat pizza. Allen likes to eat pizza.\").to_dict(),\n",
    "    config.hf : nlp(\"Allen does not like to eat pizza\").to_dict(), # test a, no , aged, handsome\n",
    "    config.idf : \"0001e\"},\n",
    "    # long change pos simple negation\n",
    "    {config.lf : config.label_to_id[\"contradiction\"],\n",
    "    config.pf : nlp(\"Tim likes to eat pizza. Tim likes to eat pizza. Tim likes to eat pizza. Allen likes to eat pizza.\").to_dict(),\n",
    "    config.hf : nlp(\"Allen does not like to eat pizza. Tim likes to eat pizza. Tim likes to eat pizza. Tim likes to eat pizza.\").to_dict(), # test a, no , aged, handsome\n",
    "    config.idf : \"0001e\"},\n",
    "    # long simple entailment\n",
    "    {config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(\"Tim likes to eat pizza. Tim likes to eat pizza. Tim likes to eat pizza. Allen likes to eat pizza.\").to_dict(),\n",
    "    config.hf : nlp(\"Allen likes to eat pizza\").to_dict(), # test a, no , aged, handsome\n",
    "    config.idf : \"0001e\"},\n",
    "]\n",
    "probe(model, data_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entailment', 'entailment', 'contradiction']\n",
      "['contradiction', 'contradiction', 'contradiction']\n"
     ]
    }
   ],
   "source": [
    "data_double_neg = [\n",
    "    # simple negation\n",
    "    {config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(\"Allen likes to eat pizza.\").to_dict(),\n",
    "    config.hf : nlp(\"Allen does not hate to eat pizza\").to_dict(), # test a, no , aged, handsome\n",
    "    config.idf : \"0001e\"},\n",
    "    # long simple negation\n",
    "    {config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(\"Allen likes to eat pizza.\").to_dict(),\n",
    "    config.hf : nlp(\"Allen does not not like to eat pizza\").to_dict(), # test a, no , aged, handsome\n",
    "    config.idf : \"0001e\"},\n",
    "    # long simple negation\n",
    "    {config.lf : config.label_to_id[\"contradiction\"],\n",
    "    config.pf : nlp(\"Allen does likes to eat pizza.\").to_dict(),\n",
    "    config.hf : nlp(\"Allen doesn't like to eat pizza\").to_dict(), # test a, no , aged, handsome\n",
    "    config.idf : \"0001e\"},\n",
    "]\n",
    "probe(model, data_double_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entailment', 'contradiction', 'entailment']\n",
      "['entailment', 'entailment', 'contradiction']\n"
     ]
    }
   ],
   "source": [
    "data_time = [\n",
    "    # simple negation\n",
    "    {config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(\"I go to school. After that, Allen not goes to school\").to_dict(),\n",
    "    config.hf : nlp(\"I go to school. After that, Allen doest not goes to school\").to_dict(), # test a, no , aged, handsome\n",
    "    config.idf : \"0001e\"},\n",
    "    # long simple negation\n",
    "    {config.lf : config.label_to_id[\"contradiction\"],\n",
    "    config.pf : nlp(\"I go to school. After that, Allen goes to school\").to_dict(),\n",
    "    config.hf : nlp(\"Allen goes to school. After that, I go to school\").to_dict(), # test a, no , aged, handsome\n",
    "    config.idf : \"0001e\"},\n",
    "    # long change pos simple negation\n",
    "    {config.lf : config.label_to_id[\"entailment\"],\n",
    "    config.pf : nlp(\"Allen is happy\").to_dict(),\n",
    "    config.hf : nlp(\"Not Allen is not happy\").to_dict(), # test a, no , aged, handsome\n",
    "    config.idf : \"0001e\"},\n",
    "]\n",
    "probe(model, data_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral', 'neutral', 'neutral']\n",
      "['entailment', 'entailment', 'contradiction']\n"
     ]
    }
   ],
   "source": [
    "data_ner = [\n",
    "    # simple negation\n",
    "    {config.lf : config.label_to_id[\"neutral\"],\n",
    "    config.pf : nlp(\"The Great Wall is a famous building\").to_dict(),\n",
    "    config.hf : nlp(\"The Great Building is a famous buildingl\").to_dict(), # test a, no , aged, handsome\n",
    "    config.idf : \"0001e\"},\n",
    "    # long simple negation\n",
    "    {config.lf : config.label_to_id[\"neutral\"],\n",
    "    config.pf : nlp(\"The Great River of China  is a famous river\").to_dict(),\n",
    "    config.hf : nlp(\"The Great River of Newyork is a famous river\").to_dict(), # test a, no , aged, handsome\n",
    "    config.idf : \"0001e\"},\n",
    "    # long change pos simple negation\n",
    "    {config.lf : config.label_to_id[\"neutral\"],\n",
    "    config.pf : nlp(\"Allen likes to eat pizza.\").to_dict(),\n",
    "    config.hf : nlp(\"Tim does not likes to eat pizza.\").to_dict(), # test a, no , aged, handsome\n",
    "    config.idf : \"0001e\"},\n",
    "]\n",
    "probe(model, data_ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MisMatch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9832/9832 [00:17<00:00, 549.37it/s]\n"
     ]
    }
   ],
   "source": [
    "testset = data.GraphDataset(config.PDEV_MMA_FILE, word2idx=word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report(model=model, dataset=None):\n",
    "    pred = []\n",
    "    label = []\n",
    "    \n",
    "    dev_loader = DataLoader(dataset, batch_size = 32, follow_batch=config.follow_batch, shuffle=False)\n",
    "    for batch_i, batch in enumerate(tqdm(dev_loader)):\n",
    "        #label.extend(torch.argmax(batch[config.label_field].cpu(), dim = 1).numpy().tolist()) # label, one hot to tensor, shape = (batch) now\n",
    "        label.extend(torch.argmax(batch.label.view([-1, config.NUM_CLASSES]), dim=1, keepdim=False).tolist())\n",
    "        for k in config.tensor_attr_list:\n",
    "                batch.__dict__[k] = batch.__dict__[k]\n",
    "        pred_batch = model._predict(batch).numpy().tolist()\n",
    "        pred.extend(pred_batch)\n",
    "        \n",
    "    report = {\n",
    "        'report' : metrics.classification_report(pred, label, output_dict=True),\n",
    "        'confusion_matrix' : metrics.confusion_matrix(pred, label)\n",
    "    }\n",
    "    return report\n",
    "\n",
    "# return list of (id, prediction, label)\n",
    "def get_errors_from_dataset(model=model, dataset=None):\n",
    "    loader = DataLoader(dataset, batch_size = 32, follow_batch=config.follow_batch, shuffle=False)\n",
    "    errors\n",
    "    from tqdm import tqdm\n",
    "    for batch in tqdm(loader):\n",
    "        pred_batch  = model._predict(batch)\n",
    "        #print(batch.label.size())\n",
    "        #print(dir(batch))\n",
    "        label_batch = torch.argmax(batch.label, dim=1, keepdim=False)\n",
    "        errors.extend([(idx, pred, label) for (idx, pred, label) in zip(batch.pid, pred_batch, label_batch) if pred != label])\n",
    "    acc = 1 - (len(errors_id) / len(dataset))\n",
    "    return errors,  acc\n",
    "\n",
    "def get_instances_by_id(errors_id, dataset):\n",
    "    instances = []\n",
    "    errors_id = set(errors_id)\n",
    "    #print(dir(dataset[0]))\n",
    "    for data in dataset:\n",
    "        if data.pid in errors_id:\n",
    "            instances.append((utils.token2sent(data.x_p, words), utils.token2sent(data.x_h, words), data.label))\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 308/308 [01:09<00:00,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.624593165174939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "errors, acc = get_errors_from_dataset(model, testset)\n",
    "print(\"accuracy is \" + str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = get_report(model, testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'report': {'0': {'precision': 0.5876543209876544,\n",
       "   'recall': 0.7245053272450532,\n",
       "   'f1-score': 0.648943421949557,\n",
       "   'support': 2628},\n",
       "  '1': {'precision': 0.6241610738255033,\n",
       "   'recall': 0.541747572815534,\n",
       "   'f1-score': 0.58004158004158,\n",
       "   'support': 3605},\n",
       "  '2': {'precision': 0.6595437481952064,\n",
       "   'recall': 0.6346207279799945,\n",
       "   'f1-score': 0.6468422543188898,\n",
       "   'support': 3599},\n",
       "  'accuracy': 0.624593165174939,\n",
       "  'macro avg': {'precision': 0.623786381002788,\n",
       "   'recall': 0.6336245426801939,\n",
       "   'f1-score': 0.6252757521033422,\n",
       "   'support': 9832},\n",
       "  'weighted avg': {'precision': 0.627354981331473,\n",
       "   'recall': 0.624593165174939,\n",
       "   'f1-score': 0.622910748802585,\n",
       "   'support': 9832}},\n",
       " 'confusion_matrix': array([[1904,  457,  267],\n",
       "        [ 740, 1953,  912],\n",
       "        [ 596,  719, 2284]])}"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7382\n"
     ]
    }
   ],
   "source": [
    "print(len(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = get_instances_by_id(errors[:10][0], devset[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:  133794c\n",
      "Premise: \n",
      "[ROOT] the answer has nothing to do with their cause , however , but with the simple fact that dictionaries are not exercises in [UNK] substitutability ; in other words , if one of the senses of run is ` operate ' ( as in she runs an engine factory ) , that does not make it valid to assume that one can substitute operate for run in we run in the marathon every year [UNK] [ROOT] although recognizing this as a shortcoming of dictionaries and assigning it arbitrarily to what , for lack of a better term , we might call the genius of the language , might seem trivial to the casual observer , it is a valid matter for concern in the realm of lexicology .\n",
      "Hypthesis: \n",
      "[ROOT] dictionaries are indeed exercises in [UNK] substitutability .\n",
      "Gold Label:  tensor([[1., 0., 0.]])\n",
      "Predicted Label:  tensor(1)\n",
      "ID:  25267e\n",
      "Premise: \n",
      "[ROOT] for ` family hold back , ' an exhortation ensuring ample provender for guests .\n",
      "Hypthesis: \n",
      "[ROOT] ' family hold back , ' an emphatic command to guarantee enough food for visitors .\n",
      "Gold Label:  tensor([[0., 0., 1.]])\n",
      "Predicted Label:  tensor(1)\n",
      "ID:  44984c\n",
      "Premise: \n",
      "[ROOT] i 'll twist him , sir .\n",
      "Hypthesis: \n",
      "[ROOT] i 'll make him straight .\n",
      "Gold Label:  tensor([[1., 0., 0.]])\n",
      "Predicted Label:  tensor(2)\n",
      "ID:  77159c\n",
      "Premise: \n",
      "[ROOT] the controller checked to see if american airlines could establish communication with american 11 .\n",
      "Hypthesis: \n",
      "[ROOT] the controller gave up trying to find a signal and declared american 11 a lost cause .\n",
      "Gold Label:  tensor([[1., 0., 0.]])\n",
      "Predicted Label:  tensor(1)\n",
      "ID:  9691n\n",
      "Premise: \n",
      "[ROOT] that means we now have the opportunity to be a stable , positive and important part of each child 's life for an entire decade .\n",
      "Hypthesis: \n",
      "[ROOT] providing stability and positivity for each child has been made possible from continued support .\n",
      "Gold Label:  tensor([[0., 1., 0.]])\n",
      "Predicted Label:  tensor(2)\n",
      "ID:  145239c\n",
      "Premise: \n",
      "[ROOT] the forecasting challenges retailers confront have been amplified in recent years by product proliferation in almost every category .\n",
      "Hypthesis: \n",
      "[ROOT] forecasting has been easier recently due to the updated process we have today .\n",
      "Gold Label:  tensor([[1., 0., 0.]])\n",
      "Predicted Label:  tensor(1)\n",
      "ID:  93664c\n",
      "Premise: \n",
      "[ROOT] the taliban leader mullah omar promptly invited bin ladin to move to kandahar , ostensibly in the interests of bin ladin 's own security but more likely to situate him where he might be easier to control .\n",
      "Hypthesis: \n",
      "[ROOT] mulah omar invited bin ladin to dallas texas .\n",
      "Gold Label:  tensor([[1., 0., 0.]])\n",
      "Predicted Label:  tensor(2)\n",
      "ID:  80532c\n",
      "Premise: \n",
      "[ROOT] captain victor saracini and first officer michael horrocks piloted the boeing 767 , which had seven flight attendants .\n",
      "Hypthesis: \n",
      "[ROOT] the captain was michael horrocks and there were 4 flight attendants aboard .\n",
      "Gold Label:  tensor([[1., 0., 0.]])\n",
      "Predicted Label:  tensor(2)\n",
      "ID:  16525n\n",
      "Premise: \n",
      "[ROOT] they were promptly executed .\n",
      "Hypthesis: \n",
      "[ROOT] they were executed immediately upon capture .\n",
      "Gold Label:  tensor([[0., 1., 0.]])\n",
      "Predicted Label:  tensor(2)\n",
      "ID:  122191n\n",
      "Premise: \n",
      "[ROOT] in all the following cases , the spelling , ( apparent ) roots , or sound of the word actively suggest a meaning different from the true one .\n",
      "Hypthesis: \n",
      "[ROOT] the word 's true meaning is derived from oral folklore .\n",
      "Gold Label:  tensor([[0., 1., 0.]])\n",
      "Predicted Label:  tensor(2)\n"
     ]
    }
   ],
   "source": [
    "for i, line in enumerate(errors[:10]):\n",
    "    print(\"ID: \" , errors_id[i])\n",
    "    print(\"Premise: \" , line[0], sep='\\n')\n",
    "    print(\"Hypthesis: \", line[1], sep='\\n')\n",
    "    print(\"Gold Label: \", line[2])\n",
    "    print(\"Predicted Label: \", errors[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
