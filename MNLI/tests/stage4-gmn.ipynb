{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1 - Code Study Allennlp + DataReader, SparseAdjacencyField (See DEV_STAGES.md for update)\n",
    "- 8/?-8/?\n",
    "- code study allennlp (across stages)\n",
    "    - study tools and resources\n",
    "        - allennlp guide\n",
    "        - allennlp doc\n",
    "        - allennlp github (source code)\n",
    "        - google\n",
    "    - data\n",
    "        - fields\n",
    "        - instances\n",
    "        - batch\n",
    "        - dataset\n",
    "        - vocabulary\n",
    "    - data operator\n",
    "        - reader(to_instance)\n",
    "        - vocab.from_instances\n",
    "        - token_indexer(index_with => token_id)\n",
    "        - dataloader(call batch_tensors in fields)\n",
    "        - embedder(token_id2vec)\n",
    "        - encoder(model part)\n",
    "    - trainer\n",
    "        - trainer\n",
    "        - tensorboard_writer\n",
    "        - config file composition and jsonnet\n",
    "    - general work flow conclusion\n",
    "        - rawdata =reader=> instance (with fields)\n",
    "        - instance =Vocab=> vocab (with namespaces)\n",
    "        - instance =IndexWith=> indexed_instances (TensorDict)\n",
    "        - instances =dataloader(batch_tensors function)=> batch_tensor (TensorDict)\n",
    "        - batch_tensor =model=> logits\n",
    "    - allennlp conclusion\n",
    "        - OOP + dependency injection\n",
    "        - a good coding style\n",
    "        - several robust off-the-shelf models\n",
    "- implementation of datareader\n",
    "    - use utils.doc2graph\n",
    "    - graph2instance\n",
    "- implementation of SparseAdjacencyField\n",
    "    - sparse version of origin AdjacencyField\n",
    "    - modify code (almost all) of allennlp AdjacencyField\n",
    "    - implementation of PytorchGeoData Batching\n",
    "\n",
    "# Stage 2 - Train a naive model (BagofWordPooling) with allennlp train\n",
    "- 8/?-8/?\n",
    "- (start this note when 2->3)\n",
    "- mismatched BERT (use defualt mean)\n",
    "    - use PretrainedTransformerMismatchedIndexer + PretrainedTransformerMismatchedEmbedder\n",
    "    - note that here use BERT without special token\n",
    "    - also \"[ROOT]\" in dependency graph is not special token to BERT is a potential issue\n",
    "- sparse2dense, dense2sparse in tensorop.py\n",
    "    - naive implementation works well without tensor\n",
    "    - fix gradient issue\n",
    "        - learn about leaf node in computatino graph\n",
    "        - inplace operation\n",
    "        - tensor properties\n",
    "        - torch.sparse.Tensor.to_dense() as tf.scatter_nd\n",
    "    - 2020/8/21, can actually use pytorch_scatter, pytorch_sparse...\n",
    "- allennlp train can work with my modules\n",
    "    \n",
    "    \n",
    "# State 3 - Train A HGNN model (het graph embedding w/o interaction)\n",
    "- due 8/22\n",
    "- add Graph2VecEncoder Registrable\n",
    "- implement HGEN\n",
    "\n",
    "# (Now) Stage 4 - Train A HGMN model (het graph matching network (may be final))\n",
    "- due 8/31\n",
    "- add GraphPair2VecEncoder Registrable\n",
    "- implement HGMN\n",
    "\n",
    "# Stage 5 - Validation on ANLI/Q-Test/HAN, Experiments\n",
    "- due 9/15\n",
    "- parse ANLI/HAN\n",
    "- Q-Test generator(This may be required earlier)\n",
    "\n",
    "# Stage 6 - Paper Fixing (due 9/19, EACL due 9/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Work, Todo\n",
    "- reader to add bidirectional relation\n",
    "    - add add_edge for simplicity\n",
    "- GraphEMbeddingNet(GraphPair2VecEncoder)\n",
    "    - todo\n",
    "- Config is modified\n",
    "    - remove or transformer embedder\n",
    "    - can train on token embedding first (quicker and see effect)\n",
    "    - also a must do exp\n",
    "- add raw_text_datareader\n",
    "- tensor_op\n",
    "    - move sparse cross attention to tensor_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/2020-IIS-NLU-internship/MNLI/tests'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## util\n",
    "import os\n",
    "import logging\n",
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm_notebook as tqdmnb\n",
    "from tqdm import tqdm as tqdm\n",
    "import pickle\n",
    "import json \n",
    "import jsonlines as jsonl\n",
    "from collections import defaultdict\n",
    "from typing import Iterable, List, Dict, Tuple, Union\n",
    "from pathlib import Path\n",
    "## graph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# geometric\n",
    "import torch_geometric\n",
    "## nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from torch_geometric.data.data import Data\n",
    "## Stanza\n",
    "import stanza\n",
    "from stanza.models.common.doc import Document\n",
    "from stanza.pipeline.core import Pipeline\n",
    "## allennlp model\n",
    "from allennlp_models.structured_prediction.predictors.srl import SemanticRoleLabelerPredictor\n",
    "from allennlp_models.structured_prediction.predictors.biaffine_dependency_parser import BiaffineDependencyParserPredictor\n",
    "from allennlp.predictors.predictor import Predictor #\n",
    "## allennlp\n",
    "from allennlp.data import Token, Vocabulary, Instance\n",
    "from allennlp.data.fields import ListField, TextField, Field\n",
    "from allennlp.data.token_indexers import (\n",
    "    SingleIdTokenIndexer,\n",
    "    TokenCharactersIndexer,\n",
    "    ELMoTokenCharactersIndexer,\n",
    "    PretrainedTransformerIndexer,\n",
    "    PretrainedTransformerMismatchedIndexer,\n",
    ")\n",
    "from allennlp.data import DatasetReader, DataLoader, Instance, Vocabulary, PyTorchDataLoader\n",
    "from allennlp.data.tokenizers import (\n",
    "    CharacterTokenizer,\n",
    "    PretrainedTransformerTokenizer,\n",
    "    SpacyTokenizer,\n",
    "    WhitespaceTokenizer,\n",
    ")\n",
    "from allennlp.modules.seq2vec_encoders import CnnEncoder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import (\n",
    "    Embedding,\n",
    "    TokenCharactersEncoder,\n",
    "    ElmoTokenEmbedder,\n",
    "    PretrainedTransformerEmbedder,\n",
    "    PretrainedTransformerMismatchedEmbedder,\n",
    ")\n",
    "from allennlp.nn import util as nn_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internal Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.config as config\n",
    "\n",
    "from src.data_git import utils as utils\n",
    "from src.data_git import reader as reader\n",
    "\n",
    "from src.models import SynNLIModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use relative by concatting pwd\n",
    "# or the cahce file name will be ..SLASH........\n",
    "bert_model = \"bert-base-uncased\"\n",
    "train_data_path = \"/work/2020-IIS-NLU-internship/MNLI/data/anli_v1.0/R1/train.jsonl\"\n",
    "validation_data_path = \"/work/2020-IIS-NLU-internship/MNLI/data/anli_v1.0/R1/dev.jsonl\"\n",
    "test_data_path = \"/work/2020-IIS-NLU-internship/MNLI/data/anli_v1.0/R1/test.jsonl\"\n",
    "cache_data_dir = \"/work/2020-IIS-NLU-internship/MNLI/data/ANLI_instance_cache/R1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from ANLI preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdr2 = reader.NLIGraphReader(input_fields=reader.config.default_fields, lazy=False, max_instances=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62fb9a8763034d1eb24b033a7f59e4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', max=1.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev2 = rdr2.read(file_path=\"../data/anli_v1.0_preprocessed/R2/dev.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc1b184416da4c53ba66d4cfee1a55b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='building vocab', style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(dev2, min_count={\"edge_labels\":500}, max_vocab_size={\"edge_labels\":20}, non_padded_namespaces= [\"*tags\", \"labels\"]) # need to use @@unlown@@ for edge labels\n",
    "# min_count={\"edge_labels\":150} => 58\n",
    "# min_count={\"edge_labels\":500} => 46\n",
    "# min_count={\"edge_labels\":1000} => 42\n",
    "# 1200 => 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev2.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader2 = PyTorchDataLoader(dev2, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary with namespaces:  edge_labels, Size: 10 || labels, Size: 3 || tags, Size: 30522 || Non Padded Namespaces: {'labels', '*tags'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO now, GraphMatchingNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_encoder = 300\n",
    "dim_embedder = 768\n",
    "dim_matching = 44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gat', 'rgcn', 'hgt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Graph2GraphEncoder.list_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bimpm']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GraphPair2GraphPairEncoder.list_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['global_attention']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Graph2VecEncoder.list_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['graph_embedding_net', 'graph_matching_net']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GraphPair2VecEncoder.list_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['edge_index', 'edge_attr', 'batch_id'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"g_p\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_embedder = PretrainedTransformerMismatchedEmbedder(model_name=config.TRANSFORMER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_nn = torch.nn.Linear(300, 1)\n",
    "node_nn = torch.nn.Linear(300, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GlobalAttention(gate_nn=Linear(in_features=300, out_features=1, bias=True), nn=Linear(in_features=300, out_features=300, bias=True))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooler = Graph2VecEncoder.by_name(\"global_attention\")(gate_nn=gate_nn, nn=node_nn)\n",
    "pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGCNConv(300, 300, num_relations=20)\n",
      "HGTConv(in_dim=300, out_dim=300, num_types=1, num_types=20)\n"
     ]
    }
   ],
   "source": [
    "rgcn = Graph2GraphEncoder.by_name(\"rgcn\")(\n",
    "    in_channels=300,\n",
    "    out_channels=300,\n",
    "    aggr=\"add\",\n",
    "    num_relations=20,\n",
    "    root_weight=False,\n",
    "    bias=False,\n",
    ")\n",
    "print(rgcn)\n",
    "\"\"\"\n",
    "\"in_dim\" : 300\n",
    "\"out_dim\" : 300\n",
    "\"num_types\" : 10\n",
    "\"num_relations\" : 20\n",
    "\"n_heads\" : 5\n",
    "\"dropout\" : 0.2\n",
    "\"use_norm\" : true\n",
    "\"use_RTE\" : false\n",
    "\"\"\"\n",
    "ght = Graph2GraphEncoder.by_name(\"hgt\")(\n",
    "    in_dim=300,\n",
    "    out_dim=300,\n",
    "    num_relations=20,\n",
    "    num_types=1,\n",
    "    n_heads=5,\n",
    "    use_RTE=False,\n",
    "    use_norm=True,\n",
    ")\n",
    "print(ght)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen = GraphPair2VecEncoder.by_name(\"graph_matching_net\")(convs=rgcn, num_layers=3, pooler=pooler) # this is a constructor\n",
    "#gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.bimpm_matching  import BiMpmMatching#from allennlp.common  import Params\n",
    "from allennlp.common import Params\n",
    "\n",
    "match = BiMpmMatching.from_params(\n",
    "    params = Params({\n",
    "        \"hidden_dim\" : 300,\n",
    "        \"num_perspectives\" : 10,\n",
    "        \"share_weights_between_directions\" : False,\n",
    "        \"with_full_match\" : False,\n",
    "        \"with_maxpool_match\" :  True,\n",
    "        \"with_attentive_match\" : True,\n",
    "        \"with_max_attentive_match\" : True,\n",
    "    })\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.modules.graph_pair2graph_pair_encoders.graph_pair_mpm import GraphPairMPM\n",
    "graph_bimpm = GraphPairMPM(bimpm=match)\n",
    "graph_bimpm._dim_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "upd = NodeUpdater.by_name(\"gru\")(input_size=dim_encoder+graph_bimpm._dim_match, hidden_size=dim_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules import FeedForward\n",
    "from allennlp.nn import Activation\n",
    "projector = FeedForward(768, 1, 300, Activation.by_name(\"linear\")(), 0.0)\n",
    "classifier = FeedForward(300*4, 2, [300, 3], Activation.by_name(\"relu\")(), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmn = GraphPair2VecEncoder.by_name(\"graph_matching_net\")(\n",
    "    num_layers = 3,\n",
    "    convs = rgcn, \n",
    "    atts = graph_bimpm,\n",
    "    updater = upd,  \n",
    "    pooler =  pooler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#print(model._modules) # no encoder of GraphMatchingNet\n",
    "print(isinstance(gmn, torch.nn.Module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SynNLIModel(\n",
    "    vocab=vocab,\n",
    "    embedder=transformer_embedder,\n",
    "    projector=projector,\n",
    "    encoder=gmn,\n",
    "    classifier=classifier,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'probs': tensor([[0.3364, 0.3262, 0.3374],\n",
       "         [0.3398, 0.3244, 0.3358]], grad_fn=<SoftmaxBackward>),\n",
       " 'loss': tensor(1.1230, grad_fn=<NllLossBackward>)}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.0, 'entropy': 1.0984586477279663}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_metrics(reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/2020-IIS-NLU-internship/MNLI/tests'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../b.pkl\", \"rb\") as fo:\n",
    "    ba = pickle.load(fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'probs': tensor([[0.3374, 0.3252, 0.3374],\n",
       "         [0.3384, 0.3251, 0.3366],\n",
       "         [0.3376, 0.3239, 0.3385],\n",
       "         [0.3385, 0.3247, 0.3368],\n",
       "         [0.3394, 0.3241, 0.3364],\n",
       "         [0.3398, 0.3239, 0.3363],\n",
       "         [0.3376, 0.3253, 0.3370],\n",
       "         [0.3386, 0.3249, 0.3365],\n",
       "         [0.3370, 0.3252, 0.3379],\n",
       "         [0.3397, 0.3248, 0.3355],\n",
       "         [0.3376, 0.3260, 0.3364],\n",
       "         [0.3372, 0.3254, 0.3375],\n",
       "         [0.3367, 0.3240, 0.3392],\n",
       "         [0.3383, 0.3242, 0.3376],\n",
       "         [0.3405, 0.3241, 0.3354],\n",
       "         [0.3377, 0.3255, 0.3368]], grad_fn=<SoftmaxBackward>),\n",
       " 'loss': tensor(1.0966, grad_fn=<NllLossBackward>)}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['token_ids', 'mask', 'type_ids', 'wordpiece_mask', 'offsets'])\n",
      "torch.Size([16, 107]) torch.Size([16, 107]) torch.Size([16, 107]) torch.Size([16, 87, 2]) torch.Size([16, 87])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(ba[\"tokens_p\"][\"tokens\"].keys())\n",
    "print(\n",
    "    ba[\"tokens_p\"][\"tokens\"][\"token_ids\"].size(),\n",
    "    ba[\"tokens_p\"][\"tokens\"][\"wordpiece_mask\"].size(),\n",
    "    ba[\"tokens_p\"][\"tokens\"][\"type_ids\"].size(),\n",
    "    ba[\"tokens_p\"][\"tokens\"][\"offsets\"].size(),\n",
    "    ba[\"tokens_p\"][\"tokens\"][\"mask\"].size()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader2.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_to_device(data, device):\n",
    "    for k in data.keys():\n",
    "        if isinstance(data[k], dict):\n",
    "            recursive_to_device(data[k], device)\n",
    "        else:\n",
    "            data[k] = data[k].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_to_device(batch, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in model.to(\"cpu\").named_parameters():\n",
    "    if n[:7] == \"encoder\":\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
