{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Now) Stage 5 - Validation on ANLI/Q-Test/HAN, Experiments\n",
    "- due 9/15\n",
    "- Text Embedding\n",
    "    - to `BasicTextFieldEmbedder`\n",
    "    - `DatasetReader` to Use ``{\"transformers\": , \"pos\": ...}``\n",
    "    - `CNN Mismatched Embedder`\n",
    "- CGConv\n",
    "    - to Test\n",
    "- Deepmind naive\n",
    "    - to do\n",
    "- Graph Generator\n",
    "    - add constituent edge\n",
    "    \n",
    "# Stage 6 - Paper Fixing (due 9/19, EACL due 9/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/2020-IIS-NLU-internship/MNLI/tests'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## util\n",
    "import os\n",
    "import logging\n",
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm_notebook as tqdmnb\n",
    "from tqdm import tqdm as tqdm\n",
    "import pickle\n",
    "import json \n",
    "import jsonlines as jsonl\n",
    "from collections import defaultdict\n",
    "from typing import Iterable, List, Dict, Tuple, Union\n",
    "from pathlib import Path\n",
    "## graph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "# geometric\n",
    "import torch_geometric\n",
    "## nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from torch_geometric.data.data import Data\n",
    "## Stanza\n",
    "import stanza\n",
    "from stanza.models.common.doc import Document\n",
    "from stanza.pipeline.core import Pipeline\n",
    "## allennlp model\n",
    "from allennlp_models.structured_prediction.predictors.srl import SemanticRoleLabelerPredictor\n",
    "from allennlp_models.structured_prediction.predictors.biaffine_dependency_parser import BiaffineDependencyParserPredictor\n",
    "from allennlp.predictors.predictor import Predictor #\n",
    "## allennlp\n",
    "from allennlp.data import Token, Vocabulary, Instance\n",
    "from allennlp.data.fields import ListField, TextField, Field\n",
    "from allennlp.data.token_indexers import (\n",
    "    SingleIdTokenIndexer,\n",
    "    TokenCharactersIndexer,\n",
    "    ELMoTokenCharactersIndexer,\n",
    "    PretrainedTransformerIndexer,\n",
    "    PretrainedTransformerMismatchedIndexer,\n",
    ")\n",
    "from allennlp.data import DatasetReader, DataLoader, Instance, Vocabulary, PyTorchDataLoader\n",
    "from allennlp.data.tokenizers import (\n",
    "    CharacterTokenizer,\n",
    "    PretrainedTransformerTokenizer,\n",
    "    SpacyTokenizer,\n",
    "    WhitespaceTokenizer,\n",
    ")\n",
    "from allennlp.modules.seq2vec_encoders import CnnEncoder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import (\n",
    "    Embedding,\n",
    "    TokenCharactersEncoder,\n",
    "    ElmoTokenEmbedder,\n",
    "    PretrainedTransformerEmbedder,\n",
    "    PretrainedTransformerMismatchedEmbedder,\n",
    ")\n",
    "from allennlp.nn import util as nn_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internal Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.config as config\n",
    "\n",
    "from src.data_git import utils as utils\n",
    "from src.data_git import reader as reader\n",
    "\n",
    "from src.models import SynNLIModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use relative by concatting pwd\n",
    "# or the cahce file name will be ..SLASH........\n",
    "bert_model = \"bert-base-uncased\"\n",
    "train_data_path = \"/work/2020-IIS-NLU-internship/MNLI/data/anli_v1.0/R1/train.jsonl\"\n",
    "validation_data_path = \"/work/2020-IIS-NLU-internship/MNLI/data/anli_v1.0/R1/dev.jsonl\"\n",
    "test_data_path = \"/work/2020-IIS-NLU-internship/MNLI/data/anli_v1.0/R1/test.jsonl\"\n",
    "cache_data_dir = \"/work/2020-IIS-NLU-internship/MNLI/data/ANLI_instance_cache/R1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read from ANLI preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdr2 = reader.NLIGraphReader(input_fields=reader.config.default_fields, lazy=False, max_instances=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17689d5a6fb42b091d9308838e715fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', max=1.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev2 = rdr2.read(file_path=\"../data/anli_v1.0_preprocessed/R2/dev.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f38939541ee4f26bed0b75d7e0ffd41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='building vocab', style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(dev2, min_count={\"edge_labels\":500}, max_vocab_size={\"edge_labels\":20}, non_padded_namespaces= [\"*tags\", \"labels\"]) # need to use @@unlown@@ for edge labels\n",
    "# min_count={\"edge_labels\":150} => 58\n",
    "# min_count={\"edge_labels\":500} => 46\n",
    "# min_count={\"edge_labels\":1000} => 42\n",
    "# 1200 => 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev2.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader2 = PyTorchDataLoader(dev2, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocabulary with namespaces:  edge_labels, Size: 12 || labels, Size: 3 || tags, Size: 30522 || Non Padded Namespaces: {'labels', '*tags'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_encoder = 300\n",
    "dim_embedder = 768\n",
    "#dim_matching = 44 # for bimpm\n",
    "dim_matching = dim_encoder # for attentive sum diff\n",
    "dim_edge = 50\n",
    "num_relations = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gat', 'rgcn', 'hgt', 'cg']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Graph2GraphEncoder.list_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bimpm', 'att_diff']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GraphPair2GraphPairEncoder.list_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['global_attention']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Graph2VecEncoder.list_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['graph_embedding_net', 'graph_matching_net']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GraphPair2VecEncoder.list_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['edge_index', 'edge_attr', 'batch_id'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"g_p\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_embedder = PretrainedTransformerMismatchedEmbedder(model_name=config.TRANSFORMER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_embedder = Embedding(\n",
    "    embedding_dim = dim_edge,\n",
    "    num_embeddings = num_relations,\n",
    "    projection_dim = None,\n",
    "    weight = None,\n",
    "    padding_index = None,\n",
    "    trainable = True,\n",
    "    max_norm = None,\n",
    "    norm_type = 2.0,\n",
    "    scale_grad_by_freq = True,\n",
    "    sparse = False,\n",
    "    vocab_namespace = \"edge_labels\",\n",
    "    pretrained_file = None,\n",
    "    vocab = vocab,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge_embedder(torch.zeros((3,1), dtype=torch.long)) #ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_nn = torch.nn.Linear(300, 1)\n",
    "node_nn = torch.nn.Linear(300, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GlobalAttention(gate_nn=Linear(in_features=300, out_features=1, bias=True), nn=Linear(in_features=300, out_features=300, bias=True))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooler = Graph2VecEncoder.by_name(\"global_attention\")(gate_nn=gate_nn, nn=node_nn)\n",
    "pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "pooler2 = deepcopy(pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(pooler) != id(pooler2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(pooler2.gate_nn) != id(pooler.gate_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGCNConv(300, 300, num_relations=20)\n",
      "HGTConv(in_dim=300, out_dim=300, num_types=1, num_types=20)\n",
      "CGConv(node_dim=300, edge_dim=50)\n"
     ]
    }
   ],
   "source": [
    "rgcn = Graph2GraphEncoder.by_name(\"rgcn\")(\n",
    "    in_channels=dim_encoder,\n",
    "    out_channels=dim_encoder,\n",
    "    aggr=\"add\",\n",
    "    num_relations=20,\n",
    "    root_weight=False,\n",
    "    bias=False,\n",
    ")\n",
    "print(rgcn)\n",
    "\"\"\"\n",
    "\"in_dim\" : 300\n",
    "\"out_dim\" : 300\n",
    "\"num_types\" : 10\n",
    "\"num_relations\" : 20\n",
    "\"n_heads\" : 5\n",
    "\"dropout\" : 0.2\n",
    "\"use_norm\" : true\n",
    "\"use_RTE\" : false\n",
    "\"\"\"\n",
    "ght = Graph2GraphEncoder.by_name(\"hgt\")(\n",
    "    in_dim=dim_encoder,\n",
    "    out_dim=dim_encoder,\n",
    "    num_relations=20,\n",
    "    num_types=1,\n",
    "    n_heads=5,\n",
    "    use_RTE=False,\n",
    "    use_norm=True,\n",
    ")\n",
    "print(ght)\n",
    "cg = Graph2GraphEncoder.by_name(\"cg\")(\n",
    "    channels = dim_encoder,\n",
    "    dim = dim_edge, #edge dim\n",
    "    aggr = 'add',\n",
    "    batch_norm = False,\n",
    "    bias = True\n",
    ")\n",
    "print(cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen = GraphPair2VecEncoder.by_name(\"graph_matching_net\")(convs=rgcn, num_layers=3, pooler=pooler) # this is a constructor\n",
    "#gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.bimpm_matching  import BiMpmMatching#from allennlp.common  import Params\n",
    "from allennlp.common import Params\n",
    "\n",
    "match = BiMpmMatching.from_params(\n",
    "    params = Params({\n",
    "        \"hidden_dim\" : dim_encoder,\n",
    "        \"num_perspectives\" : 10,\n",
    "        \"share_weights_between_directions\" : False,\n",
    "        \"with_full_match\" : False,\n",
    "        \"with_maxpool_match\" :  True,\n",
    "        \"with_attentive_match\" : True,\n",
    "        \"with_max_attentive_match\" : True,\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphPairMPM\n",
    "from src.modules.graph_pair2graph_pair_encoders.graph_pair_mpm import GraphPairMPM\n",
    "graph_bimpm = GraphPairMPM(bimpm=match)\n",
    "# AttentiveSumDiff\n",
    "from src.modules.graph_pair2graph_pair_encoders import AttentiveSumDiff\n",
    "from allennlp.modules import MatrixAttention\n",
    "mat_att = MatrixAttention.by_name(\"cosine\")()\n",
    "att_diff = AttentiveSumDiff(\n",
    "    att = mat_att,\n",
    "    dim = dim_encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 2 different updaters\n",
    "upd = NodeUpdater.by_name(\"gru\")(input_size=dim_encoder+dim_matching, hidden_size=dim_encoder)\n",
    "upd2 = NodeUpdater.by_name(\"gru\")(input_size=dim_encoder+dim_matching, hidden_size=dim_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules import FeedForward\n",
    "from allennlp.nn import Activation\n",
    "projector = FeedForward(768, 1, 300, Activation.by_name(\"linear\")(), 0.0)\n",
    "classifier = FeedForward(300*4, 2, [300, 3], Activation.by_name(\"relu\")(), 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmn = GraphPair2VecEncoder.by_name(\"graph_matching_net\")(\n",
    "    num_layers = 3,\n",
    "    convs = cg, \n",
    "    atts = att_diff,\n",
    "    updaters = [upd, upd2], # use tuple in json ok? seems not, use list instead\n",
    "    poolers =  [pooler, pooler2],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#print(model._modules) # no encoder of GraphMatchingNet\n",
    "print(isinstance(gmn, torch.nn.Module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper ver text field embedder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "bte = BasicTextFieldEmbedder(\n",
    "    token_embedders = {\"tokens\": transformer_embedder}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import GraphNLIModel\n",
    "model = GraphNLIModel(\n",
    "    vocab=vocab,\n",
    "    embedder=bte,\n",
    "    edge_embedder=edge_embedder,\n",
    "    projector=projector,\n",
    "    encoder=gmn,\n",
    "    classifier=classifier,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'probs': tensor([[0.3314, 0.3328, 0.3358],\n",
       "         [0.3253, 0.3280, 0.3467]], grad_fn=<SoftmaxBackward>),\n",
       " 'loss': tensor(1.1075, grad_fn=<NllLossBackward>)}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.0, 'entropy': 1.098402500152588}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_metrics(reset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/2020-IIS-NLU-internship/MNLI/tests'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../b.pkl\", \"rb\") as fo:\n",
    "    ba = pickle.load(fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model(**ba, return_attention=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test GPU device can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_to_device(data, device):\n",
    "    for k in data.keys():\n",
    "        if isinstance(data[k], dict):\n",
    "            recursive_to_device(data[k], device)\n",
    "        else:\n",
    "            data[k] = data[k].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_to_device(batch, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphNLIModel(\n",
       "  (embedder): BasicTextFieldEmbedder(\n",
       "    (token_embedder_tokens): PretrainedTransformerMismatchedEmbedder(\n",
       "      (_matched_embedder): PretrainedTransformerEmbedder(\n",
       "        (transformer_model): BertModel(\n",
       "          (embeddings): BertEmbeddings(\n",
       "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "            (position_embeddings): Embedding(512, 768)\n",
       "            (token_type_embeddings): Embedding(2, 768)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (encoder): BertEncoder(\n",
       "            (layer): ModuleList(\n",
       "              (0): BertLayer(\n",
       "                (attention): BertAttention(\n",
       "                  (self): BertSelfAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): BertSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): BertIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                )\n",
       "                (output): BertOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (1): BertLayer(\n",
       "                (attention): BertAttention(\n",
       "                  (self): BertSelfAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): BertSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): BertIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                )\n",
       "                (output): BertOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (2): BertLayer(\n",
       "                (attention): BertAttention(\n",
       "                  (self): BertSelfAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): BertSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): BertIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                )\n",
       "                (output): BertOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (3): BertLayer(\n",
       "                (attention): BertAttention(\n",
       "                  (self): BertSelfAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): BertSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): BertIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                )\n",
       "                (output): BertOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (4): BertLayer(\n",
       "                (attention): BertAttention(\n",
       "                  (self): BertSelfAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): BertSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): BertIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                )\n",
       "                (output): BertOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (5): BertLayer(\n",
       "                (attention): BertAttention(\n",
       "                  (self): BertSelfAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): BertSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): BertIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                )\n",
       "                (output): BertOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (6): BertLayer(\n",
       "                (attention): BertAttention(\n",
       "                  (self): BertSelfAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): BertSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): BertIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                )\n",
       "                (output): BertOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (7): BertLayer(\n",
       "                (attention): BertAttention(\n",
       "                  (self): BertSelfAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): BertSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): BertIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                )\n",
       "                (output): BertOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (8): BertLayer(\n",
       "                (attention): BertAttention(\n",
       "                  (self): BertSelfAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): BertSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): BertIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                )\n",
       "                (output): BertOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (9): BertLayer(\n",
       "                (attention): BertAttention(\n",
       "                  (self): BertSelfAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): BertSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): BertIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                )\n",
       "                (output): BertOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (10): BertLayer(\n",
       "                (attention): BertAttention(\n",
       "                  (self): BertSelfAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): BertSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): BertIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                )\n",
       "                (output): BertOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (11): BertLayer(\n",
       "                (attention): BertAttention(\n",
       "                  (self): BertSelfAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (output): BertSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): BertIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                )\n",
       "                (output): BertOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (pooler): BertPooler(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (activation): Tanh()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (edge_embedder): Embedding()\n",
       "  (projector): FeedForward(\n",
       "    (_activations): ModuleList(\n",
       "      (0): Linear()\n",
       "    )\n",
       "    (_linear_layers): ModuleList(\n",
       "      (0): Linear(in_features=768, out_features=300, bias=True)\n",
       "    )\n",
       "    (_dropout): ModuleList(\n",
       "      (0): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder): GraphMatchingNet(\n",
       "    (_convs): ModuleList(\n",
       "      (0): CGConv(node_dim=300, edge_dim=50)\n",
       "      (1): CGConv(node_dim=300, edge_dim=50)\n",
       "      (2): CGConv(node_dim=300, edge_dim=50)\n",
       "    )\n",
       "    (_atts): ModuleList(\n",
       "      (0): (CosineMatrixAttention(), dim_matching_output=300)\n",
       "      (1): (CosineMatrixAttention(), dim_matching_output=300)\n",
       "      (2): (CosineMatrixAttention(), dim_matching_output=300)\n",
       "    )\n",
       "    (_updaters): ModuleList(\n",
       "      (0): GRUNodeUpdater(\n",
       "        (_rnn): GRUCell(600, 300)\n",
       "      )\n",
       "      (1): GRUNodeUpdater(\n",
       "        (_rnn): GRUCell(600, 300)\n",
       "      )\n",
       "    )\n",
       "    (_poolers): ModuleList(\n",
       "      (0): GlobalAttention(gate_nn=Linear(in_features=300, out_features=1, bias=True), nn=Linear(in_features=300, out_features=300, bias=True))\n",
       "      (1): GlobalAttention(gate_nn=Linear(in_features=300, out_features=1, bias=True), nn=Linear(in_features=300, out_features=300, bias=True))\n",
       "    )\n",
       "  )\n",
       "  (classifier): FeedForward(\n",
       "    (_activations): ModuleList(\n",
       "      (0): ReLU()\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (_linear_layers): ModuleList(\n",
       "      (0): Linear(in_features=1200, out_features=300, bias=True)\n",
       "      (1): Linear(in_features=300, out_features=3, bias=True)\n",
       "    )\n",
       "    (_dropout): ModuleList(\n",
       "      (0): Dropout(p=0.0, inplace=False)\n",
       "      (1): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'probs': tensor([[0.3314, 0.3328, 0.3358],\n",
       "         [0.3253, 0.3280, 0.3467]], device='cuda:0', grad_fn=<SoftmaxBackward>),\n",
       " 'loss': tensor(1.1075, device='cuda:0', grad_fn=<NllLossBackward>)}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in model.to(\"cpu\").named_parameters():\n",
    "    if n[:7] == \"encoder\":\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-07 22:56:36 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2020-09-07 22:56:36 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "| lemma     | ewt     |\n",
      "| depparse  | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-09-07 22:56:36 INFO: Use device: gpu\n",
      "2020-09-07 22:56:36 INFO: Loading: tokenize\n",
      "2020-09-07 22:56:36 INFO: Loading: pos\n",
      "2020-09-07 22:56:37 INFO: Loading: lemma\n",
      "2020-09-07 22:56:37 INFO: Loading: depparse\n",
      "2020-09-07 22:56:38 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from src.data_git.reader import NLIGraphReader\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse')\n",
    "rdr_nlp = NLIGraphReader(\n",
    "        wordpiece_tokenizer = None,\n",
    "        token_indexers = None,\n",
    "        combine_input_fields = None,\n",
    "        input_parsed  = False,\n",
    "        parser = nlp,\n",
    "        input_fields = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.predictors import GraphNLIPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = GraphNLIPredictor(model = model, dataset_reader=rdr_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'probs': [0.3287544548511505, 0.3287544548511505, 0.34249114990234375],\n",
       " 'loss': 1.0715094804763794,\n",
       " 'predicted_label': 'c'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict_json(\n",
    "{\n",
    "    \"sentence1\": \"Allen is smart.\",\n",
    "    \"sentence2\" : \"Allen is stupid.\",\n",
    "    \"gold_label\": \"c\",\n",
    "})\n",
    "# to be done, need to pass nlp some where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'probs': [0.33141055703163147, 0.3327915072441101, 0.33579787611961365], 'loss': 1.1002390384674072, 'predicted_label': 'c'}\n"
     ]
    }
   ],
   "source": [
    "print(predictor.predict_instance(dev2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'probs': [0.33141055703163147, 0.3327915072441101, 0.33579787611961365],\n",
       "  'predicted_label': 'c'},\n",
       " {'probs': [0.32532286643981934, 0.3279840350151062, 0.3466930389404297],\n",
       "  'predicted_label': 'c'}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict_batch_instance([dev2[0], dev2[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.interpret import (\n",
    "    show_sequence_attention,\n",
    "    show_matrix_attention,\n",
    "    AttentionVisualizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = AttentionVisualizer(model=model, reader=rdr_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-07 22:59:08 INFO: tokens_p are ['Allen', 'is', 'a', 'dog', '.']\n",
      "2020-09-07 22:59:08 INFO: tokens_h are ['Allen', 'is', 'a', 'cat', '.']\n",
      "2020-09-07 22:59:08 INFO: the predicted label is ['c']\n",
      "2020-09-07 22:59:08 INFO: the gold label is c\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vis.visualize_json({\n",
    "    \"sentence1\": \"Allen is a dog.\",\n",
    "    \"sentence2\": \"Allen is a cat.\",\n",
    "    \"gold_label\": \"c\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Loading + Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialization_dir = \"../param/GMN_BERT_300d_/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common import Params\n",
    "lm_config = Params.from_file(serialization_dir+\"config.json\")\n",
    "lm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lm = Model.from_archive(\"../param\n",
    "lm = Model.load(\n",
    "    config = lm_config,\n",
    "    serialization_dir = serialization_dir,\n",
    "    weights_file = None, # use best by default\n",
    "    cuda_device = -1\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_vocab = lm.vocab\n",
    "lm_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_load = GraphNLIPredictor(model = lm, dataset_reader=rdr_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att1h = lm(**batch, return_attention=True)[\"attentions\"][\"pooler2\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att1p = lm(**batch, return_attention=True)[\"attentions\"][\"pooler1\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sequence_attention(str1p, att1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sequence_attention(str1h, att1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_inst(inst):\n",
    "    print(\n",
    "        inst.fields[\"tokens_p\"],\n",
    "        inst.fields[\"tokens_h\"],\n",
    "        inst.fields[\"label\"],\n",
    "        sep='\\n',\n",
    "        )\n",
    "print_inst(dev2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_load.predict_batch_instance([dev2[0], dev2[1]]) # can return label(after modify code!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two ways for model loading \n",
    "# params of config + .th\n",
    "\n",
    "\"\"\"\n",
    "components = run_config(CONFIG)\n",
    "params = components['params']\n",
    "dataset_reader = components['dataset_reader']\n",
    "vocab = components['vocab']\n",
    "model = components['model']\n",
    "\n",
    "\n",
    "original_preds = make_predictions(model, dataset_reader)\n",
    "\n",
    "# Save the model\n",
    "serialization_dir = 'model'\n",
    "config_file = os.path.join(serialization_dir, 'config.json')\n",
    "vocabulary_dir = os.path.join(serialization_dir, 'vocabulary')\n",
    "weights_file = os.path.join(serialization_dir, 'weights.th')\n",
    "\n",
    "os.makedirs(serialization_dir, exist_ok=True)\n",
    "params.to_file(config_file)\n",
    "vocab.save_to_files(vocabulary_dir)\n",
    "torch.save(model.state_dict(), weights_file)\n",
    "\n",
    "# Load the model\n",
    "loaded_params = Params.from_file(config_file)\n",
    "loaded_model = Model.load(loaded_params, serialization_dir, weights_file)\n",
    "loaded_vocab = loaded_model.vocab   # Vocabulary is loaded in Model.load()\n",
    "\n",
    "# Make sure the predictions are the same\n",
    "loaded_preds = make_predictions(loaded_model, dataset_reader)\n",
    "assert original_preds == loaded_preds\n",
    "print('predictions matched')\n",
    "\n",
    "# Create an archive file\n",
    "archive_model(serialization_dir, weights='weights.th')\n",
    "\n",
    "# Unarchive from the file\n",
    "archive = load_archive(os.path.join(serialization_dir, 'model.tar.gz'))\n",
    "\n",
    "# Make sure the predictions are the same\n",
    "archived_preds = make_predictions(archive.model, dataset_reader)\n",
    "assert original_preds == archived_preds\n",
    "print('predictions matched')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_git.utils import text2graph, doc2graph\n",
    "print(text2graph(\"The Big Apple is Newyork City\", nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
