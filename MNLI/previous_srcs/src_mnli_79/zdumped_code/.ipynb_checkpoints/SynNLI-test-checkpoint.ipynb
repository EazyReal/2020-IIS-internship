{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SynNLI v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor #\n",
    "import allennlp_models.structured_prediction\n",
    "import allennlp_models.coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "from allennlp.data.fields import TextField, LabelField, SequenceLabelField\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.data.instance import Instance\n",
    "\n",
    "from typing import Iterable\n",
    "import logging\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_srl = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/bert-base-srl-2020.03.24.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbs': [{'verb': 'think', 'description': 'Did [ARG0: Uriah] [ARGM-MNR: honestly] [V: think] [ARG1: he could beat the game in under three hours] ?', 'tags': ['O', 'B-ARG0', 'B-ARGM-MNR', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O']}, {'verb': 'could', 'description': 'Did Uriah honestly think he [V: could] beat the game in under three hours ?', 'tags': ['O', 'O', 'O', 'O', 'O', 'B-V', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'verb': 'beat', 'description': 'Did Uriah honestly think [ARG0: he] [ARGM-MOD: could] [V: beat] [ARG1: the game] in [ARGM-TMP: under three hours] ?', 'tags': ['O', 'O', 'O', 'O', 'B-ARG0', 'B-ARGM-MOD', 'B-V', 'B-ARG1', 'I-ARG1', 'O', 'B-ARGM-TMP', 'I-ARGM-TMP', 'I-ARGM-TMP', 'O']}], 'words': ['Did', 'Uriah', 'honestly', 'think', 'he', 'could', 'beat', 'the', 'game', 'in', 'under', 'three', 'hours', '?']}\n"
     ]
    }
   ],
   "source": [
    "doc = predictor_srl.predict(\n",
    "  sentence=\"Did Uriah honestly think he could beat the game in under three hours?\"\n",
    ")\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Did not use initialization regex that was passed: .*weight_hh.*\n",
      "Did not use initialization regex that was passed: .*projection.*weight\n",
      "Did not use initialization regex that was passed: .*weight_ih.*\n",
      "Did not use initialization regex that was passed: .*bias_hh.*\n",
      "Did not use initialization regex that was passed: .*projection.*bias\n",
      "Did not use initialization regex that was passed: .*bias_ih.*\n"
     ]
    }
   ],
   "source": [
    "predictor_dep = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/biaffine-dependency-parser-ptb-2020.04.06.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your label namespace was 'pos'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arc_loss': 0.37905168533325195, 'tag_loss': 0.5370486974716187, 'loss': 0.9161003828048706, 'words': ['If', 'I', 'bring', '10', 'dollars', 'tomorrow', ',', 'can', 'you', 'buy', 'me', 'lunch', '?'], 'pos': ['SCONJ', 'PRON', 'VERB', 'NUM', 'NOUN', 'NOUN', 'PUNCT', 'VERB', 'PRON', 'VERB', 'PRON', 'NOUN', 'PUNCT'], 'predicted_dependencies': ['mark', 'nsubj', 'advcl', 'dep', 'dobj', 'tmod', 'advmod', 'aux', 'nsubj', 'root', 'dobj', 'dep', 'discourse'], 'predicted_heads': [3, 3, 10, 5, 3, 3, 10, 10, 10, 0, 10, 11, 10], 'hierplane_tree': {'text': 'If I bring 10 dollars tomorrow , can you buy me lunch ?', 'root': {'word': 'buy', 'nodeType': 'root', 'attributes': ['VERB'], 'link': 'root', 'spans': [{'start': 41, 'end': 45}], 'children': [{'word': 'bring', 'nodeType': 'advcl', 'attributes': ['VERB'], 'link': 'advcl', 'spans': [{'start': 5, 'end': 11}], 'children': [{'word': 'If', 'nodeType': 'mark', 'attributes': ['SCONJ'], 'link': 'mark', 'spans': [{'start': 0, 'end': 3}]}, {'word': 'I', 'nodeType': 'nsubj', 'attributes': ['PRON'], 'link': 'nsubj', 'spans': [{'start': 3, 'end': 5}]}, {'word': 'dollars', 'nodeType': 'dobj', 'attributes': ['NOUN'], 'link': 'dobj', 'spans': [{'start': 14, 'end': 22}], 'children': [{'word': '10', 'nodeType': 'dep', 'attributes': ['NUM'], 'link': 'dep', 'spans': [{'start': 11, 'end': 14}]}]}, {'word': 'tomorrow', 'nodeType': 'tmod', 'attributes': ['NOUN'], 'link': 'tmod', 'spans': [{'start': 22, 'end': 31}]}]}, {'word': ',', 'nodeType': 'advmod', 'attributes': ['PUNCT'], 'link': 'advmod', 'spans': [{'start': 31, 'end': 33}]}, {'word': 'can', 'nodeType': 'aux', 'attributes': ['VERB'], 'link': 'aux', 'spans': [{'start': 33, 'end': 37}]}, {'word': 'you', 'nodeType': 'nsubj', 'attributes': ['PRON'], 'link': 'nsubj', 'spans': [{'start': 37, 'end': 41}]}, {'word': 'me', 'nodeType': 'dobj', 'attributes': ['PRON'], 'link': 'dobj', 'spans': [{'start': 45, 'end': 48}], 'children': [{'word': 'lunch', 'nodeType': 'dep', 'attributes': ['NOUN'], 'link': 'dep', 'spans': [{'start': 48, 'end': 54}]}]}, {'word': '?', 'nodeType': 'discourse', 'attributes': ['PUNCT'], 'link': 'discourse', 'spans': [{'start': 54, 'end': 56}]}]}, 'nodeTypeToStyle': {'root': ['color5', 'strong'], 'dep': ['color5', 'strong'], 'nsubj': ['color1'], 'nsubjpass': ['color1'], 'csubj': ['color1'], 'csubjpass': ['color1'], 'pobj': ['color2'], 'dobj': ['color2'], 'iobj': ['color2'], 'mark': ['color2'], 'pcomp': ['color2'], 'xcomp': ['color2'], 'ccomp': ['color2'], 'acomp': ['color2'], 'aux': ['color3'], 'cop': ['color3'], 'det': ['color3'], 'conj': ['color3'], 'cc': ['color3'], 'prep': ['color3'], 'number': ['color3'], 'possesive': ['color3'], 'poss': ['color3'], 'discourse': ['color3'], 'expletive': ['color3'], 'prt': ['color3'], 'advcl': ['color3'], 'mod': ['color4'], 'amod': ['color4'], 'tmod': ['color4'], 'quantmod': ['color4'], 'npadvmod': ['color4'], 'infmod': ['color4'], 'advmod': ['color4'], 'appos': ['color4'], 'nn': ['color4'], 'neg': ['color0'], 'punct': ['color0']}, 'linkToPosition': {'nsubj': 'left', 'nsubjpass': 'left', 'csubj': 'left', 'csubjpass': 'left', 'pobj': 'right', 'dobj': 'right', 'iobj': 'right', 'pcomp': 'right', 'xcomp': 'right', 'ccomp': 'right', 'acomp': 'right'}}}\n",
      "\n",
      " ['If', 'I', 'bring', '10', 'dollars', 'tomorrow', ',', 'can', 'you', 'buy', 'me', 'lunch', '?'] [3, 3, 10, 5, 3, 3, 10, 10, 10, 0, 10, 11, 10]\n"
     ]
    }
   ],
   "source": [
    "doc = predictor_dep.predict(\n",
    "  sentence=\"If I bring 10 dollars tomorrow, can you buy me lunch?\"\n",
    ")\n",
    "print(doc)\n",
    "print(\"\\n\", doc[\"words\"], doc[\"predicted_heads\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Did not use initialization regex that was passed: _context_layer._module.weight_ih.*\n",
      "Did not use initialization regex that was passed: _context_layer._module.weight_hh.*\n"
     ]
    }
   ],
   "source": [
    "predictor_coref = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'top_spans': [[0, 4], [5, 5], [7, 8], [10, 10], [10, 11]],\n",
       " 'antecedent_indices': [[0, 1, 2, 3, 4],\n",
       "  [0, 1, 2, 3, 4],\n",
       "  [0, 1, 2, 3, 4],\n",
       "  [0, 1, 2, 3, 4],\n",
       "  [0, 1, 2, 3, 4]],\n",
       " 'predicted_antecedents': [-1, -1, -1, 0, -1],\n",
       " 'document': ['The',\n",
       "  'woman',\n",
       "  'reading',\n",
       "  'a',\n",
       "  'newspaper',\n",
       "  'sat',\n",
       "  'on',\n",
       "  'the',\n",
       "  'bench',\n",
       "  'with',\n",
       "  'her',\n",
       "  'dog',\n",
       "  '.'],\n",
       " 'clusters': [[[0, 4], [10, 10]]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_coref.predict(\n",
    "  document=\"The woman reading a newspaper sat on the bench with her dog.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@DatasetReader.register('nli-jsonl')\n",
    "class NLI_Jsonl_Reader(DatasetReader):\n",
    "    def __init__(self,\n",
    "                 tokenizer: Tokenizer = None,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 max_tokens: int = None,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.tokenizer = tokenizer or WhitespaceTokenizer()\n",
    "        self.token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def dict_to_instance(self, jdata: dict, label: str = None) -> Instance:\n",
    "        p, h, l = jdata[config.pf], jdata[config.hf], jdata[config.lf]\n",
    "        p_tokens = self.tokenizer.tokenize(p)\n",
    "        h_tokens = self.tokenizer.tokenize(h)\n",
    "        if self.max_tokens:\n",
    "            p_tokens = p_tokens[:self.max_tokens]\n",
    "            h_tokens = h_tokens[:self.max_tokens]\n",
    "        text_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {config.pf: text_field, config.hf: }\n",
    "        if label:\n",
    "            fields['label'] = LabelField(label)\n",
    "        return Instance(fields)\n",
    "\n",
    "    def _read(self, file_path: str) -> Iterable[Instance]:\n",
    "        with jsonlines.open(file_path, \"r\") as fo:\n",
    "            for jdata in fo.iter():\n",
    "                yield self.dict_to_instance(jdata)\n",
    "                #yield self.text_to_instance(premise, hypothesis, sentiment)\n",
    "\n",
    "\n",
    "# Instantiate and use the dataset reader to read a file containing the data\n",
    "reader = ClassificationTsvReader()\n",
    "dataset = reader.read('quick_start/data/movie_review/train.tsv')\n",
    "\n",
    "# Returned dataset is a list of Instances by default\n",
    "print('type of dataset: ', type(dataset))\n",
    "print('type of its first element: ', type(dataset[0]))\n",
    "print('size of dataset: ', len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'allennlp.modules.matrix_attention.legacy_matrix_attention'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-362579f82591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeedForward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputVariationalDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_matrix_attention\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLegacyMatrixAttention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeq2SeqEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimilarityFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextFieldEmbedder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInitializerApplicator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRegularizerApplicator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'allennlp.modules.matrix_attention.legacy_matrix_attention'"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Optional, List, Any\n",
    "\n",
    "import torch\n",
    "\n",
    "from allennlp.common.checks import check_dimensions_match\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.modules import FeedForward, InputVariationalDropout\n",
    "from allennlp.modules.matrix_attention.legacy_matrix_attention import LegacyMatrixAttention\n",
    "from allennlp.modules import Seq2SeqEncoder, SimilarityFunction, TextFieldEmbedder\n",
    "from allennlp.nn import InitializerApplicator, RegularizerApplicator\n",
    "from allennlp.nn.util import get_text_field_mask, last_dim_softmax, weighted_sum, replace_masked_values\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "\n",
    "\n",
    "@Model.register(\"esim\")\n",
    "class ESIM(Model):\n",
    "    \"\"\"\n",
    "    This ``Model`` implements the ESIM sequence model described in `\"Enhanced LSTM for Natural Language Inference\"\n",
    "    <https://www.semanticscholar.org/paper/Enhanced-LSTM-for-Natural-Language-Inference-Chen-Zhu/83e7654d545fbbaaf2328df365a781fb67b841b4>`_\n",
    "    by Chen et al., 2017.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab : ``Vocabulary``\n",
    "    text_field_embedder : ``TextFieldEmbedder``\n",
    "        Used to embed the ``premise`` and ``hypothesis`` ``TextFields`` we get as input to the\n",
    "        model.\n",
    "    encoder : ``Seq2SeqEncoder``\n",
    "        Used to encode the premise and hypothesis.\n",
    "    similarity_function : ``SimilarityFunction``\n",
    "        This is the similarity function used when computing the similarity matrix between encoded\n",
    "        words in the premise and words in the hypothesis.\n",
    "    projection_feedforward : ``FeedForward``\n",
    "        The feedforward network used to project down the encoded and enhanced premise and hypothesis.\n",
    "    inference_encoder : ``Seq2SeqEncoder``\n",
    "        Used to encode the projected premise and hypothesis for prediction.\n",
    "    output_feedforward : ``FeedForward``\n",
    "        Used to prepare the concatenated premise and hypothesis for prediction.\n",
    "    output_logit : ``FeedForward``\n",
    "        This feedforward network computes the output logits.\n",
    "    dropout : ``float``, optional (default=0.5)\n",
    "        Dropout percentage to use.\n",
    "    initializer : ``InitializerApplicator``, optional (default=``InitializerApplicator()``)\n",
    "        Used to initialize the model parameters.\n",
    "    regularizer : ``RegularizerApplicator``, optional (default=``None``)\n",
    "        If provided, will be used to calculate the regularization penalty during training.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 encoder: Seq2SeqEncoder,\n",
    "                 similarity_function: SimilarityFunction,\n",
    "                 projection_feedforward: FeedForward,\n",
    "                 inference_encoder: Seq2SeqEncoder,\n",
    "                 output_feedforward: FeedForward,\n",
    "                 output_logit: FeedForward,\n",
    "                 dropout: float = 0.5,\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 regularizer: Optional[RegularizerApplicator] = None) -> None:\n",
    "        super().__init__(vocab, regularizer)\n",
    "\n",
    "        self._text_field_embedder = text_field_embedder\n",
    "        self._encoder = encoder\n",
    "\n",
    "        self._matrix_attention = LegacyMatrixAttention(similarity_function)\n",
    "        self._projection_feedforward = projection_feedforward\n",
    "\n",
    "        self._inference_encoder = inference_encoder\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = torch.nn.Dropout(dropout)\n",
    "            self.rnn_input_dropout = InputVariationalDropout(dropout)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "            self.rnn_input_dropout = None\n",
    "\n",
    "        self._output_feedforward = output_feedforward\n",
    "        self._output_logit = output_logit\n",
    "\n",
    "        self._num_labels = vocab.get_vocab_size(namespace=\"labels\")\n",
    "\n",
    "        check_dimensions_match(text_field_embedder.get_output_dim(), encoder.get_input_dim(),\n",
    "                               \"text field embedding dim\", \"encoder input dim\")\n",
    "        check_dimensions_match(encoder.get_output_dim() * 4, projection_feedforward.get_input_dim(),\n",
    "                               \"encoder output dim\", \"projection feedforward input\")\n",
    "        check_dimensions_match(projection_feedforward.get_output_dim(), inference_encoder.get_input_dim(),\n",
    "                               \"proj feedforward output dim\", \"inference lstm input dim\")\n",
    "\n",
    "        self._accuracy = CategoricalAccuracy()\n",
    "        self._loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        initializer(self)\n",
    "\n",
    "    def forward(self,  # type: ignore\n",
    "                premise: Dict[str, torch.LongTensor],\n",
    "                hypothesis: Dict[str, torch.LongTensor],\n",
    "                label: torch.IntTensor = None,\n",
    "                metadata: List[Dict[str, Any]] = None  # pylint:disable=unused-argument\n",
    "               ) -> Dict[str, torch.Tensor]:\n",
    "        # pylint: disable=arguments-differ\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        premise : Dict[str, torch.LongTensor]\n",
    "            From a ``TextField``\n",
    "        hypothesis : Dict[str, torch.LongTensor]\n",
    "            From a ``TextField``\n",
    "        label : torch.IntTensor, optional (default = None)\n",
    "            From a ``LabelField``\n",
    "        metadata : ``List[Dict[str, Any]]``, optional, (default = None)\n",
    "            Metadata containing the original tokenization of the premise and\n",
    "            hypothesis with 'premise_tokens' and 'hypothesis_tokens' keys respectively.\n",
    "        Returns\n",
    "        -------\n",
    "        An output dictionary consisting of:\n",
    "        label_logits : torch.FloatTensor\n",
    "            A tensor of shape ``(batch_size, num_labels)`` representing unnormalised log\n",
    "            probabilities of the entailment label.\n",
    "        label_probs : torch.FloatTensor\n",
    "            A tensor of shape ``(batch_size, num_labels)`` representing probabilities of the\n",
    "            entailment label.\n",
    "        loss : torch.FloatTensor, optional\n",
    "            A scalar loss to be optimised.\n",
    "        \"\"\"\n",
    "        embedded_premise = self._text_field_embedder(premise)\n",
    "        embedded_hypothesis = self._text_field_embedder(hypothesis)\n",
    "        premise_mask = get_text_field_mask(premise).float()\n",
    "        hypothesis_mask = get_text_field_mask(hypothesis).float()\n",
    "\n",
    "        # apply dropout for LSTM\n",
    "        if self.rnn_input_dropout:\n",
    "            embedded_premise = self.rnn_input_dropout(embedded_premise)\n",
    "            embedded_hypothesis = self.rnn_input_dropout(embedded_hypothesis)\n",
    "\n",
    "        # encode premise and hypothesis\n",
    "        encoded_premise = self._encoder(embedded_premise, premise_mask)\n",
    "        encoded_hypothesis = self._encoder(embedded_hypothesis, hypothesis_mask)\n",
    "\n",
    "        # Shape: (batch_size, premise_length, hypothesis_length)\n",
    "        similarity_matrix = self._matrix_attention(encoded_premise, encoded_hypothesis)\n",
    "\n",
    "        # Shape: (batch_size, premise_length, hypothesis_length)\n",
    "        p2h_attention = last_dim_softmax(similarity_matrix, hypothesis_mask)\n",
    "        # Shape: (batch_size, premise_length, embedding_dim)\n",
    "        attended_hypothesis = weighted_sum(encoded_hypothesis, p2h_attention)\n",
    "\n",
    "        # Shape: (batch_size, hypothesis_length, premise_length)\n",
    "        h2p_attention = last_dim_softmax(similarity_matrix.transpose(1, 2).contiguous(), premise_mask)\n",
    "        # Shape: (batch_size, hypothesis_length, embedding_dim)\n",
    "        attended_premise = weighted_sum(encoded_premise, h2p_attention)\n",
    "\n",
    "        # the \"enhancement\" layer\n",
    "        premise_enhanced = torch.cat(\n",
    "                [encoded_premise, attended_hypothesis,\n",
    "                 encoded_premise - attended_hypothesis,\n",
    "                 encoded_premise * attended_hypothesis],\n",
    "                dim=-1\n",
    "        )\n",
    "        hypothesis_enhanced = torch.cat(\n",
    "                [encoded_hypothesis, attended_premise,\n",
    "                 encoded_hypothesis - attended_premise,\n",
    "                 encoded_hypothesis * attended_premise],\n",
    "                dim=-1\n",
    "        )\n",
    "\n",
    "        # The projection layer down to the model dimension.  Dropout is not applied before\n",
    "        # projection.\n",
    "        projected_enhanced_premise = self._projection_feedforward(premise_enhanced)\n",
    "        projected_enhanced_hypothesis = self._projection_feedforward(hypothesis_enhanced)\n",
    "\n",
    "        # Run the inference layer\n",
    "        if self.rnn_input_dropout:\n",
    "            projected_enhanced_premise = self.rnn_input_dropout(projected_enhanced_premise)\n",
    "            projected_enhanced_hypothesis = self.rnn_input_dropout(projected_enhanced_hypothesis)\n",
    "        v_ai = self._inference_encoder(projected_enhanced_premise, premise_mask)\n",
    "        v_bi = self._inference_encoder(projected_enhanced_hypothesis, hypothesis_mask)\n",
    "\n",
    "        # The pooling layer -- max and avg pooling.\n",
    "        # (batch_size, model_dim)\n",
    "        v_a_max, _ = replace_masked_values(\n",
    "                v_ai, premise_mask.unsqueeze(-1), -1e7\n",
    "        ).max(dim=1)\n",
    "        v_b_max, _ = replace_masked_values(\n",
    "                v_bi, hypothesis_mask.unsqueeze(-1), -1e7\n",
    "        ).max(dim=1)\n",
    "\n",
    "        v_a_avg = torch.sum(v_ai * premise_mask.unsqueeze(-1), dim=1) / torch.sum(\n",
    "                premise_mask, 1, keepdim=True\n",
    "        )\n",
    "        v_b_avg = torch.sum(v_bi * hypothesis_mask.unsqueeze(-1), dim=1) / torch.sum(\n",
    "                hypothesis_mask, 1, keepdim=True\n",
    "        )\n",
    "\n",
    "        # Now concat\n",
    "        # (batch_size, model_dim * 2 * 4)\n",
    "        v_all = torch.cat([v_a_avg, v_a_max, v_b_avg, v_b_max], dim=1)\n",
    "\n",
    "        # the final MLP -- apply dropout to input, and MLP applies to output & hidden\n",
    "        if self.dropout:\n",
    "            v_all = self.dropout(v_all)\n",
    "\n",
    "        output_hidden = self._output_feedforward(v_all)\n",
    "        label_logits = self._output_logit(output_hidden)\n",
    "        label_probs = torch.nn.functional.softmax(label_logits, dim=-1)\n",
    "\n",
    "        output_dict = {\"label_logits\": label_logits, \"label_probs\": label_probs}\n",
    "\n",
    "        if label is not None:\n",
    "            loss = self._loss(label_logits, label.long().view(-1))\n",
    "            self._accuracy(label_logits, label)\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {'accuracy': self._accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
