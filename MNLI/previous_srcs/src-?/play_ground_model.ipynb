{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader, Model, Trainning\n",
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0708 02:29:03.224430 140051427272512 file_utils.py:39] PyTorch version 1.5.1+cu101 available.\n",
      "I0708 02:29:04.914098 140051427272512 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    }
   ],
   "source": [
    "from dataset import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = True\n",
    "LOG = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "n_class = 2\n",
    "PRETRAINED_MODEL_NAME = 'bert-base-chinese'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Path ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = './FGC_release_1.7.13/FGC_release_all_dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default tolkenizer is used\n",
    "train_set = FGC_Dataset(train_data_path, \"train\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '苏', '东', '坡', '在', '中', '国', '历', '史', '上', '，', '是', '哪', '一', '个', '朝', '代', '的', '人', '？', '[SEP]', '苏', '轼', '（', '103', '##7', '年', '1', '月', '8', '日', '－', '110', '##1', '年', '8', '月', '24', '日', '）', '，', '[SEP]'] \n",
      " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) \n",
      " tensor(True)\n"
     ]
    }
   ],
   "source": [
    "if TEST or LOG:\n",
    "    print(train_set.tokenizer.convert_ids_to_tokens(train_set[0][0]), \"\\n\",\n",
    "    train_set[0][1],\"\\n\",\n",
    "    train_set[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "# collate_fn : list of instances to minibatch\n",
    "trainloader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    tokens_tensors.shape   = torch.Size([64, 55]) \n",
      "    tensor([[ 101, 5722,  691,  ...,    0,    0,    0],\n",
      "        [ 101, 5722,  691,  ...,    0,    0,    0],\n",
      "        [ 101, 5722,  691,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 5722,  691,  ...,    0,    0,    0],\n",
      "        [ 101, 5722,  691,  ...,    0,    0,    0],\n",
      "        [ 101, 5722,  691,  ...,    0,    0,    0]])\n",
      "    ------------------------\n",
      "    segments_tensors.shape = torch.Size([64, 76])\n",
      "    tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "    ------------------------\n",
      "    masks_tensors.shape    = torch.Size([64, 55])\n",
      "    tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "    ------------------------\n",
      "    label_ids.shape        = torch.Size([64])\n",
      "    tensor([ True, False,  True, False,  True, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False,  True,  True, False, False,\n",
      "         True, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False])\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "if TEST:\n",
    "    data = next(iter(trainloader))\n",
    "\n",
    "    tokens_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "\n",
    "    print(f\"\"\"\n",
    "    tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "    {tokens_tensors}\n",
    "    ------------------------\n",
    "    segments_tensors.shape = {segments_tensors.shape}\n",
    "    {segments_tensors}\n",
    "    ------------------------\n",
    "    masks_tensors.shape    = {masks_tensors.shape}\n",
    "    {masks_tensors}\n",
    "    ------------------------\n",
    "    label_ids.shape        = {label_ids.shape}\n",
    "    {label_ids}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547\n",
      "8843\n",
      "tensor([16.1664,  1.0659])\n",
      "tensor([15.1664,  1.0000])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# calc pos weight for BCE\n",
    "total = 0\n",
    "true_cnt = 0\n",
    "for instance in train_set:\n",
    "    if(instance[-1] == True):\n",
    "        true_cnt += 1\n",
    "    total += 1\n",
    "print(true_cnt)\n",
    "print(total)\n",
    "print(torch.tensor([total/true_cnt, total/(total-true_cnt)]))\n",
    "# to increase the value of recall in the model's criterion\n",
    "pos_weight = print(torch.tensor([(total-true_cnt)/true_cnt, 1]))\n",
    "print(pos_weight)\n",
    "# no need to applied pos_weight = torch.tensor([total/true_cnt, total/(1-true_cnt)])?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0707 03:40:08.506779 139996915132224 configuration_utils.py:264] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json from cache at /root/.cache/torch/transformers/8a3b1cfe5da58286e12a0f5d7d182b8d6eca88c08e26c332ee3817548cf7e60a.f12a4f986e43d8b328f5b067a641064d67b91597567a06c7b122d1ca7dfd9741\n",
      "I0707 03:40:08.509397 139996915132224 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "I0707 03:40:08.547540 139996915132224 modeling_utils.py:667] loading weights file https://cdn.huggingface.co/bert-base-chinese-pytorch_model.bin from cache at /root/.cache/torch/transformers/a75f2e45a9463e784dfe8c1d9672440d5fc1b091d5ab104e3c2d82e90ab1b222.929717ca66a3ba9eb9ec2f85973c6398c54c38a4faa464636a491d7a705f7eb6\n",
      "I0707 03:40:10.923027 139996915132224 modeling_utils.py:765] All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "I0707 03:40:10.925879 139996915132224 modeling_utils.py:774] All the weights of BertModel were initialized from the model checkpoint at bert-base-chinese.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSERModel(nn.Module):\n",
    "    \"\"\"\n",
    "    baseline\n",
    "    naive bert by NSP stype + linear classifier applied on [CLS] last hidden\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bert_encoder=None):\n",
    "        super(BertSERModel, self).__init__()\n",
    "        if bert_encoder == None or not isinstance(bert_encoder, BertModel):\n",
    "            print(\"unkown bert model choice, init with PRETRAINED_MODEL_NAME\")\n",
    "            self.bert_encoder = BertModel.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "        self.bert_encoder = bert_encoder\n",
    "        self.dropout = nn.Dropout(p=bert_encoder.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(bert_encoder.config.hidden_size, 1)\n",
    "        # critrion add positive weight\n",
    "        self.criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    def forward_nn(self, batch):\n",
    "        \"\"\"\n",
    "        batch[0] = input\n",
    "        batch[1] = token_type_ids (which sent)\n",
    "        batch[2] = mask for padding\n",
    "        \"\"\"\n",
    "        # the _ here is the last hidden states\n",
    "        # q_poolout is a 768-d vector of [CLS]\n",
    "        _, q_poolout = self.bert_encoder(batch[0],\n",
    "                                         token_type_ids=batch[1],\n",
    "                                         attention_mask=batch[2])\n",
    "        # q_poolout = self.dropout(q_poolout), MT Wu : no dropout better, without \n",
    "        logits = self.classifier(q_poolout)\n",
    "        # can apply nn.module.Sigmoid here to convert to p-distribution\n",
    "        # score is indeed better (and more stable)\n",
    "        logits = logits.squeeze(-1)\n",
    "        return logits\n",
    "    \n",
    "    # the nn.Module method\n",
    "    def forward(self, batch):\n",
    "        logits = self.forward_nn(batch)\n",
    "        loss = self.criterion(logits, batch['label'])\n",
    "        return loss\n",
    "    \n",
    "    # return sigmoded score\n",
    "    def _predict(self, batch):\n",
    "        logits = self.forward_nn(batch)\n",
    "        scores = torch.sigmoid(logits)\n",
    "        scores = scores.cpu().numpy().tolist()\n",
    "        return scores\n",
    "    \n",
    "    # return result with assigned threshold, default = 0.5\n",
    "    def predict_fgc(self, q_batch, threshold=0.5):\n",
    "        scores = self._predict(q_batch)\n",
    "\n",
    "        max_i = 0\n",
    "        max_score = 0\n",
    "        sp = []\n",
    "        for i, score in enumerate(scores):\n",
    "            if score > max_score:\n",
    "                max_i = i\n",
    "                max_score = score\n",
    "            if score >= threshold:\n",
    "                sp.append(i)\n",
    "\n",
    "        if not sp:\n",
    "            sp.append(max_i)\n",
    "\n",
    "        return {'sp': sp, 'sp_scores': scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_line_model = BertSERModel(bert_model)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "整個分類模型的參數量：102268417\n",
      "線性分類器的參數量：769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]\n",
    "     \n",
    "model_params = get_learnable_params(base_line_model)\n",
    "clf_params = get_learnable_params(base_line_model.classifier)\n",
    "\n",
    "print(f\"\"\"\n",
    "整個分類模型的參數量：{sum(p.numel() for p in model_params)}\n",
    "線性分類器的參數量：{sum(p.numel() for p in clf_params)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "CPU times: user 194 µs, sys: 39 µs, total: 233 µs\n",
      "Wall time: 165 µs\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
