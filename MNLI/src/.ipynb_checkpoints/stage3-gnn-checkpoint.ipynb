{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1 - Code Study Allennlp + DataReader, SparseAdjacencyField\n",
    "- 8/?-8/?\n",
    "- code study allennlp (across stages)\n",
    "    - study tools and resources\n",
    "        - allennlp guide\n",
    "        - allennlp doc\n",
    "        - allennlp github (source code)\n",
    "        - google\n",
    "    - data\n",
    "        - fields\n",
    "        - instances\n",
    "        - batch\n",
    "        - dataset\n",
    "        - vocabulary\n",
    "    - data operator\n",
    "        - reader(to_instance)\n",
    "        - vocab.from_instances\n",
    "        - token_indexer(index_with => token_id)\n",
    "        - dataloader(call batch_tensors in fields)\n",
    "        - embedder(token_id2vec)\n",
    "        - encoder(model part)\n",
    "    - trainer\n",
    "        - trainer\n",
    "        - tensorboard_writer\n",
    "        - config file composition and jsonnet\n",
    "    - general work flow conclusion\n",
    "        - rawdata =reader=> instance (with fields)\n",
    "        - instance =Vocab=> vocab (with namespaces)\n",
    "        - instance =IndexWith=> indexed_instances (TensorDict)\n",
    "        - instances =dataloader(batch_tensors function)=> batch_tensor (TensorDict)\n",
    "        - batch_tensor =model=> logits\n",
    "    - allennlp conclusion\n",
    "        - OOP + dependency injection\n",
    "        - a good coding style\n",
    "        - several robust off-the-shelf models\n",
    "- implementation of datareader\n",
    "    - use utils.doc2graph\n",
    "    - graph2instance\n",
    "- implementation of SparseAdjacencyField\n",
    "    - sparse version of origin AdjacencyField\n",
    "    - modify code (almost all) of allennlp AdjacencyField\n",
    "    - implementation of PytorchGeoData Batching\n",
    "\n",
    "# Stage 2 - Train a naive model (BagofWordPooling) with allennlp train\n",
    "- 8/?-8/?\n",
    "- (start this note when 2->3)\n",
    "- mismatched BERT (use defualt mean)\n",
    "    - use PretrainedTransformerMismatchedIndexer + PretrainedTransformerMismatchedEmbedder\n",
    "    - note that here use BERT without special token\n",
    "    - also \"[ROOT]\" in dependency graph is not special token to BERT is a potential issue\n",
    "- sparse2dense, dense2sparse in tensorop.py\n",
    "    - naive implementation works well without tensor\n",
    "    - fix gradient issue\n",
    "        - learn about leaf node in computatino graph\n",
    "        - inplace operation\n",
    "        - tensor properties\n",
    "        - torch.sparse.Tensor.to_dense() as tf.scatter_nd\n",
    "    - 2020/8/21, can actually use pytorch_scatter, pytorch_sparse...\n",
    "- allennlp train can work with my modules\n",
    "    \n",
    "    \n",
    "# (Now) State 3 - Train A HGNN model (het graph embedding w/o interaction)\n",
    "- due 8/22\n",
    "- add Graph2VecEncoder Registrable\n",
    "- implement HGEN\n",
    "\n",
    "# Stage 4 - Train A HGMN model (het graph matching network (may be final))\n",
    "- due 8/31\n",
    "- add GraphPair2VecEncoder Registrable\n",
    "- implement HGMN\n",
    "\n",
    "# Stage 5 - Validation on ANLI/Q-Test/HAN, Experiments\n",
    "- due 9/15\n",
    "- parse ANLI/HAN\n",
    "- Q-Test generator(This may be required earlier)\n",
    "\n",
    "# Stage 6 - Paper Fixing (due 9/19, EACL due 9/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Work, Todo\n",
    "- reader to add bidirectional relation\n",
    "    - add add_edge for simplicity\n",
    "- GraphEMbeddingNet(GraphPair2VecEncoder)\n",
    "    - todo\n",
    "- Config is modified\n",
    "    - remove or transformer embedder\n",
    "    - can train on token embedding first (quicker and see effect)\n",
    "    - also a must do exp\n",
    "- add raw_text_datareader\n",
    "- tensor_op\n",
    "    - move sparse cross attention to tensor_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import utils\n",
    "import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use relative by concatting pwd\n",
    "# or the cahce file name will be ..SLASH........\n",
    "bert_model = \"bert-base-uncased\"\n",
    "train_data_path = \"/work/2020-IIS-NLU-internship/MNLI/data/data/anli_v1.0/R1/train.jsonl\"\n",
    "validation_data_path = \"/work/2020-IIS-NLU-internship/MNLI/data/anli_v1.0/R1/dev.jsonl\"\n",
    "test_data_path = \"/work/2020-IIS-NLU-internship/MNLI/data/anli_v1.0/R1/test.jsonl\"\n",
    "cache_data_dir = \"/work/2020-IIS-NLU-internship/MNLI/data/ANLI_instance_cache/R1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 120kB [00:00, 407kB/s]                     \n",
      "2020-08-22 18:50:40 INFO: Downloading default packages for language: en (English)...\n",
      "2020-08-22 18:50:41 INFO: File exists: /root/stanza_resources/en/default.zip.\n",
      "2020-08-22 18:50:47 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
      "2020-08-22 18:50:47 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2020-08-22 18:50:47 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "| lemma     | ewt     |\n",
      "| depparse  | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-08-22 18:50:47 INFO: Use device: gpu\n",
      "2020-08-22 18:50:47 INFO: Loading: tokenize\n",
      "2020-08-22 18:50:52 INFO: Loading: pos\n",
      "2020-08-22 18:50:53 INFO: Loading: lemma\n",
      "2020-08-22 18:50:53 INFO: Loading: depparse\n",
      "2020-08-22 18:50:55 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdr = reader.NLIGraphReader(input_parsed=False, parser=nlp, cache_directory=cache_data_dir, input_fields=reader.config.anli_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdr._input_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839850e59d0c4db9a43410513ed02b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', max=1.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eace371bca14873af167083f24ca750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='caching instances', max=1000.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<allennlp.data.dataset_readers.dataset_reader.AllennlpDataset at 0x7ff4e055ee10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdr.read(file_path=validation_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4003f5ab8a34042a0b54ecc50f176ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', max=1.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev_set = rdr.read(file_path=\"../data/anli_v1.0/R1/dev.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ROOT] Linguistics is the scientific study of language , and involves an analysis of language form , language meaning , and language in context . [ROOT] The earliest activities in the documentation and description of language have been attributed to the 4th century BCE Indian grammarian Pāṇini , who wrote a formal description of the Sanskrit language in his \" Aṣṭādhyāy ī \" .'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([str(token) for token in dev_set[0].fields[\"tokens_p\"].tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457c42a533ed4093b06d54ab15895cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', max=1.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3d2a06639f4e00a1b3a8fcf9937d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='caching instances', max=16946.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = rdr.read(file_path=train_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc3a48c38174033a8ee2bf0b9dafe7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', max=1.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12aa2a34580c496c823c8e20b6731e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='caching instances', max=1000.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_set = rdr.read(file_path=\"/work/2020-IIS-NLU-internship/MNLI/data/anli_v1.0/R1/test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/2020-IIS-NLU-internship/MNLI/src_gmn'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16946\n"
     ]
    }
   ],
   "source": [
    "with open(\"/work/2020-IIS-NLU-internship/MNLI/data/anli_v1.0/R1/train.jsonl\", \"r\") as fo:\n",
    "    print(len(fo.readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
