{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1 - Code Study Allennlp + DataReader, SparseAdjacencyField\n",
    "- 8/?-8/?\n",
    "- code study allennlp (across stages)\n",
    "    - study tools and resources\n",
    "        - allennlp guide\n",
    "        - allennlp doc\n",
    "        - allennlp github (source code)\n",
    "        - google\n",
    "    - data\n",
    "        - fields\n",
    "        - instances\n",
    "        - batch\n",
    "        - dataset\n",
    "        - vocabulary\n",
    "    - data operator\n",
    "        - reader(to_instance)\n",
    "        - vocab.from_instances\n",
    "        - token_indexer(index_with => token_id)\n",
    "        - dataloader(call batch_tensors in fields)\n",
    "        - embedder(token_id2vec)\n",
    "        - encoder(model part)\n",
    "    - trainer\n",
    "        - trainer\n",
    "        - tensorboard_writer\n",
    "        - config file composition and jsonnet\n",
    "    - general work flow conclusion\n",
    "        - rawdata =reader=> instance (with fields)\n",
    "        - instance =Vocab=> vocab (with namespaces)\n",
    "        - instance =IndexWith=> indexed_instances (TensorDict)\n",
    "        - instances =dataloader(batch_tensors function)=> batch_tensor (TensorDict)\n",
    "        - batch_tensor =model=> logits\n",
    "    - allennlp conclusion\n",
    "        - OOP + dependency injection\n",
    "        - a good coding style\n",
    "        - several robust off-the-shelf models\n",
    "- implementation of datareader\n",
    "    - use utils.doc2graph\n",
    "    - graph2instance\n",
    "- implementation of SparseAdjacencyField\n",
    "    - sparse version of origin AdjacencyField\n",
    "    - modify code (almost all) of allennlp AdjacencyField\n",
    "    - implementation of PytorchGeoData Batching\n",
    "\n",
    "# Stage 2 - Train a naive model (BagofWordPooling) with allennlp train\n",
    "- 8/?-8/?\n",
    "- (start this note when 2->3)\n",
    "- mismatched BERT (use defualt mean)\n",
    "    - use PretrainedTransformerMismatchedIndexer + PretrainedTransformerMismatchedEmbedder\n",
    "    - note that here use BERT without special token\n",
    "    - also \"[ROOT]\" in dependency graph is not special token to BERT is a potential issue\n",
    "- sparse2dense, dense2sparse in tensorop.py\n",
    "    - naive implementation works well without tensor\n",
    "    - fix gradient issue\n",
    "        - learn about leaf node in computatino graph\n",
    "        - inplace operation\n",
    "        - tensor properties\n",
    "        - torch.sparse.Tensor.to_dense() as tf.scatter_nd\n",
    "    - 2020/8/21, can actually use pytorch_scatter, pytorch_sparse...\n",
    "- allennlp train can work with my modules\n",
    "    \n",
    "    \n",
    "# (Now) State 3 - Train A HGNN model (het graph embedding w/o interaction)\n",
    "- due 8/22\n",
    "- add Graph2VecEncoder Registrable\n",
    "- implement HGEN\n",
    "\n",
    "# Stage 4 - Train A HGMN model (het graph matching network (may be final))\n",
    "- due 8/31\n",
    "- add GraphPair2VecEncoder Registrable\n",
    "- implement HGMN\n",
    "\n",
    "# Stage 5 - Validation on ANLI/Q-Test/HAN, Experiments\n",
    "- due 9/15\n",
    "- parse ANLI/HAN\n",
    "- Q-Test generator(This may be required earlier)\n",
    "\n",
    "# Stage 6 - Paper Fixing (due 9/19, EACL due 9/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Work, Todo\n",
    "- reader to add bidirectional relation\n",
    "    - add add_edge for simplicity\n",
    "- GraphEMbeddingNet(GraphPair2VecEncoder)\n",
    "    - todo\n",
    "- Config is modified\n",
    "    - remove or transformer embedder\n",
    "    - can train on token embedding first (quicker and see effect)\n",
    "    - also a must do exp\n",
    "- add raw_text_datareader\n",
    "- tensor_op\n",
    "    - move sparse cross attention to tensor_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import utils\n",
    "import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = \"bert-base-uncased\"\n",
    "train_data_path = \"../data/anli_v1.0/R1/train.jsonl\"\n",
    "validation_data_path = \"../data/anli_v1.0/R1/dev.jsonl\"\n",
    "test_data_path = \"../data/anli_v1.0/R1/test.jsonl\"\n",
    "cache_data_dir = \"../data/ANLI_instance_cache\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.0.0.json: 120kB [00:00, 885kB/s]                     \n",
      "2020-08-22 18:03:36 INFO: Downloading default packages for language: en (English)...\n",
      "2020-08-22 18:03:38 INFO: File exists: /root/stanza_resources/en/default.zip.\n",
      "2020-08-22 18:03:44 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
      "2020-08-22 18:03:44 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2020-08-22 18:03:44 INFO: Loading these models for language: en (English):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ewt     |\n",
      "| pos       | ewt     |\n",
      "| lemma     | ewt     |\n",
      "| depparse  | ewt     |\n",
      "=======================\n",
      "\n",
      "2020-08-22 18:03:44 INFO: Use device: gpu\n",
      "2020-08-22 18:03:44 INFO: Loading: tokenize\n",
      "2020-08-22 18:03:48 INFO: Loading: pos\n",
      "2020-08-22 18:03:50 INFO: Loading: lemma\n",
      "2020-08-22 18:03:50 INFO: Loading: depparse\n",
      "2020-08-22 18:03:51 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdr = reader.NLIGraphReader(input_parsed=False, parser=nlp, cache_directory=cache_data_dir, input_fields=reader.config.anli_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688f906fdeed4fa5a061d3f7e24de998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', max=1.0, style=Progâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'premise'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-89e9f2dbb068>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/allennlp/data/dataset_readers/dataset_reader.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;31m# Then some validation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                 \u001b[0minstances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/2020-IIS-NLU-internship/MNLI/src_gmn/reader.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_to_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mpremise\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mStanzaDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                 \u001b[0mhypothesis\u001b[0m \u001b[0;34m:\u001b[0m  \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mStanzaDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_to_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpremise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'premise'"
     ]
    }
   ],
   "source": [
    "rdr.read(file_path=validation_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
