{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import utils\n",
    "import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## util\n",
    "import os\n",
    "import logging\n",
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm_notebook as tqdmnb\n",
    "from tqdm import tqdm as tqdm\n",
    "import pickle\n",
    "import json \n",
    "import jsonlines as jsonl\n",
    "from collections import defaultdict\n",
    "from typing import Iterable, List, Dict, Tuple, Union\n",
    "from pathlib import Path\n",
    "## graph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "## nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.utils.convert import to_networkx\n",
    "from torch_geometric.data.data import Data\n",
    "## Stanza\n",
    "import stanza\n",
    "from stanza.models.common.doc import Document\n",
    "from stanza.pipeline.core import Pipeline\n",
    "## allennlp model\n",
    "from allennlp_models.structured_prediction.predictors.srl import SemanticRoleLabelerPredictor\n",
    "from allennlp_models.structured_prediction.predictors.biaffine_dependency_parser import BiaffineDependencyParserPredictor\n",
    "from allennlp.predictors.predictor import Predictor #\n",
    "## allennlp\n",
    "from allennlp.data import Token, Vocabulary, Instance\n",
    "from allennlp.data.fields import ListField, TextField, Field\n",
    "from allennlp.data.token_indexers import (\n",
    "    SingleIdTokenIndexer,\n",
    "    TokenCharactersIndexer,\n",
    "    ELMoTokenCharactersIndexer,\n",
    "    PretrainedTransformerIndexer,\n",
    "    PretrainedTransformerMismatchedIndexer,\n",
    ")\n",
    "from allennlp.data import DatasetReader, DataLoader, Instance, Vocabulary, PyTorchDataLoader\n",
    "from allennlp.data.tokenizers import (\n",
    "    CharacterTokenizer,\n",
    "    PretrainedTransformerTokenizer,\n",
    "    SpacyTokenizer,\n",
    "    WhitespaceTokenizer,\n",
    ")\n",
    "from allennlp.modules.seq2vec_encoders import CnnEncoder\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import (\n",
    "    Embedding,\n",
    "    TokenCharactersEncoder,\n",
    "    ElmoTokenEmbedder,\n",
    "    PretrainedTransformerEmbedder,\n",
    "    PretrainedTransformerMismatchedEmbedder,\n",
    ")\n",
    "from allennlp.nn import util as nn_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/2020-IIS-NLU-internship/MNLI/data/MNLI_Stanza/pre_multinli_1.0_dev_mismatched.jsonl\n"
     ]
    }
   ],
   "source": [
    "file_path = config.P_TEST_FILE\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparse_adjacency_field import SparseAdjacencyField as SF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygd = Data(edge_index=torch.tensor([[0,1], [1,0]]), edge_attr=[\"e0\", \"e1\"])\n",
    "pygd\n",
    "seqf = TextField([Token(\"v0\"), Token(\"vi\")], token_indexers={\"tokens\":SingleIdTokenIndexer(namespace='tokens')})\n",
    "seqf2 = TextField([Token(\"v2\"), Token(\"vi\"), Token(\"v0\")], token_indexers={\"tokens\":SingleIdTokenIndexer(namespace='tokens')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[3], edge_index=[2, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pygd2 = Data(edge_index=torch.tensor([[0,1, 2], [1,0,2]]), edge_attr=[\"e2\", \"e1\", \"e3\"])\n",
    "pygd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaf = SF(pygd, sequence_field=seqf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaf2 = SF(pygd2, sequence_field=seqf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e483c87dcc64b1d9da50329344f0a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='building vocab', max=2.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab2 = Vocabulary.from_instances(instances=[spaf, spaf2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'edge_labels'}\n"
     ]
    }
   ],
   "source": [
    "#bat = Batch([spaf, spaf2])\n",
    "print(vocab2.get_namespaces())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaf.index(vocab2)\n",
    "spaf2.index(vocab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'edge_attr': tensor([1, 0]), 'edge_index': tensor([[0, 1],\n",
      "        [1, 0]]), 'batch_id': tensor([0., 0.])}\n",
      "{'edge_attr': tensor([2, 0, 3]), 'edge_index': tensor([[0, 1, 2],\n",
      "        [1, 0, 2]]), 'batch_id': tensor([0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "t1 = spaf.as_tensor(spaf.get_padding_lengths())\n",
    "t2 = spaf2.as_tensor(spaf.get_padding_lengths())\n",
    "print(t1, t2, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'edge_index': tensor([[0, 1, 2, 3, 4, 5, 6],\n",
       "         [1, 0, 3, 2, 4, 6, 5]]),\n",
       " 'edge_attr': tensor([1, 0, 2, 0, 3, 1, 0]),\n",
       " 'batch_id': tensor([0, 0, 1, 1, 1, 2, 2])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bt3 = spaf.batch_tensors([t1, t2, t1])\n",
    "bt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sparse_adjacency_field.SparseAdjacencyField at 0x7f81fd4ff530>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spaf.empty_field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del reader \n",
    "import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdr = reader.NLI_Graph_Reader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb24da80ad846e8acc33028f29a9b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='reading instances', max=1.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dev_set = rdr.read(config.P_DEV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abe2aaf3e5646ff93786e0e97939faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='building vocab', max=9815.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----Vocabulary Statistics----\n",
      "\n",
      "\n",
      "Top 10 most frequent tokens in namespace 'edge_labels':\n",
      "\tToken: punct\t\tFrequency: 32065\n",
      "\tToken: case\t\tFrequency: 30338\n",
      "\tToken: nsubj\t\tFrequency: 29153\n",
      "\tToken: det\t\tFrequency: 28436\n",
      "\tToken: root\t\tFrequency: 20496\n",
      "\tToken: advmod\t\tFrequency: 17098\n",
      "\tToken: amod\t\tFrequency: 16704\n",
      "\tToken: obj\t\tFrequency: 15775\n",
      "\tToken: obl\t\tFrequency: 14845\n",
      "\tToken: compound\t\tFrequency: 13444\n",
      "\n",
      "Top 10 longest tokens in namespace 'edge_labels':\n",
      "\tToken: compound:prt\t\tlength: 12\tFrequency: 1258\n",
      "\tToken: nsubj:pass\t\tlength: 10\tFrequency: 2960\n",
      "\tToken: det:predet\t\tlength: 10\tFrequency: 256\n",
      "\tToken: nmod:npmod\t\tlength: 10\tFrequency: 140\n",
      "\tToken: cc:preconj\t\tlength: 10\tFrequency: 122\n",
      "\tToken: reparandum\t\tlength: 10\tFrequency: 10\n",
      "\tToken: csubj:pass\t\tlength: 10\tFrequency: 7\n",
      "\tToken: nmod:poss\t\tlength: 9\tFrequency: 5120\n",
      "\tToken: acl:relcl\t\tlength: 9\tFrequency: 3045\n",
      "\tToken: discourse\t\tlength: 9\tFrequency: 2649\n",
      "\n",
      "Top 10 shortest tokens in namespace 'edge_labels':\n",
      "\tToken: cc\t\tlength: 2\tFrequency: 9585\n",
      "\tToken: acl\t\tlength: 3\tFrequency: 2750\n",
      "\tToken: cop\t\tlength: 3\tFrequency: 7964\n",
      "\tToken: aux\t\tlength: 3\tFrequency: 9263\n",
      "\tToken: obl\t\tlength: 3\tFrequency: 14845\n",
      "\tToken: obj\t\tlength: 3\tFrequency: 15775\n",
      "\tToken: det\t\tlength: 3\tFrequency: 28436\n",
      "\tToken: list\t\tlength: 4\tFrequency: 57\n",
      "\tToken: iobj\t\tlength: 4\tFrequency: 234\n",
      "\tToken: expl\t\tlength: 4\tFrequency: 1357\n"
     ]
    }
   ],
   "source": [
    "vocab.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader = PyTorchDataLoader(dev_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dev_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['edge_index', 'edge_attr', 'batch_id'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"g_p\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (gmn.py, line 64)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3331\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-33-b7450210bd4f>\"\u001b[0m, line \u001b[1;32m1\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    import model\n",
      "\u001b[0;36m  File \u001b[0;32m\"/work/2020-IIS-NLU-internship/MNLI/src_gmn/model.py\"\u001b[0;36m, line \u001b[0;32m23\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from gmn import GraphMatchingNetwork\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/work/2020-IIS-NLU-internship/MNLI/src_gmn/gmn.py\"\u001b[0;36m, line \u001b[0;32m64\u001b[0m\n\u001b[0;31m    raise ValueError('n_blocks (%s) has to be an integer.' % str(n_blocks))\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.token_embedders import (\n",
    "    Embedding,\n",
    "    TokenCharactersEncoder,\n",
    "    ElmoTokenEmbedder,\n",
    "    PretrainedTransformerEmbedder,\n",
    "    PretrainedTransformerMismatchedEmbedder,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_embedder = PretrainedTransformerMismatchedEmbedder(\n",
    "    model_name=config.TRANSFORMER_NAME,\n",
    "    max_length=None, # concat if over max len (512 for BERT base)\n",
    "    train_parameters=True,\n",
    "    last_layer_only=True,\n",
    "    gradient_checkpointing=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Use this embedder to embed wordpieces given by `PretrainedTransformerMismatchedIndexer`\n",
      "    and to pool the resulting vectors to get word-level representations.\n",
      "\n",
      "    Registered as a `TokenEmbedder` with name \"pretrained_transformer_mismatchd\".\n",
      "\n",
      "    # Parameters\n",
      "\n",
      "    model_name : `str`\n",
      "        The name of the `transformers` model to use. Should be the same as the corresponding\n",
      "        `PretrainedTransformerMismatchedIndexer`.\n",
      "    max_length : `int`, optional (default = `None`)\n",
      "        If positive, folds input token IDs into multiple segments of this length, pass them\n",
      "        through the transformer model independently, and concatenate the final representations.\n",
      "        Should be set to the same value as the `max_length` option on the\n",
      "        `PretrainedTransformerMismatchedIndexer`.\n",
      "    train_parameters: `bool`, optional (default = `True`)\n",
      "        If this is `True`, the transformer weights get updated during training.\n",
      "    last_layer_only: `bool`, optional (default = `True`)\n",
      "        When `True` (the default), only the final layer of the pretrained transformer is taken\n",
      "        for the embeddings. But if set to `False`, a scalar mix of all of the layers\n",
      "        is used.\n",
      "    gradient_checkpointing: `bool`, optional (default = `None`)\n",
      "        Enable or disable gradient checkpointing.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(PretrainedTransformerMismatchedEmbedder.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = transformer_embedder(**batch[\"tokens_p\"][\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tokens'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens_p\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 96, 768])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp.size() # just as wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sparse_adjacency_field.SparseAdjacencyField at 0x7f81fd4ff290>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spaf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['token_ids', 'mask', 'type_ids', 'wordpiece_mask', 'offsets'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens_p\"][\"tokens\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 96])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens_p\"][\"tokens\"][\"mask\"].size() # this is for building sprase batch for Gconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
