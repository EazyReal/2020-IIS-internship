{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "import torch\n",
    "import util\n",
    "import random\n",
    "import pickle\n",
    "import glob\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = False\n",
    "\n",
    "input_size = 1024\n",
    "hidden_size = 300\n",
    "label_smooth = False\n",
    "eps = 0\n",
    "bert = 'bert-large-uncased'\n",
    "folder = 'split_dataset/rest_mask'\n",
    "bidirection = True\n",
    "if binary:\n",
    "    num_classes = 2\n",
    "else:\n",
    "    num_classes = 3\n",
    "dropout = 0.7\n",
    "att_dropout = 0\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "shuffle = True\n",
    "layer = 3\n",
    "heads = 6\n",
    "batch_size = 32\n",
    "embed_trainable = False\n",
    "max_aspect_length = 10\n",
    "l2 = 1e-4\n",
    "add_self = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts,train_targets,train_labels = util.readACL(os.path.join(folder,'restaurants_train.txt'),binary)\n",
    "'''if label_smooth:\n",
    "    for i in range(len(train_labels)):\n",
    "        a = [0.05,0.05,0.05]\n",
    "        a[train_labels[i]] = 0.9\n",
    "        train_labels[i] = a\n",
    "        '''\n",
    "\n",
    "dev_texts,dev_targets,dev_labels = util.readACL(os.path.join(folder,'restaurants_dev.txt'),binary)#Restaurants\n",
    "train_texts+= dev_texts\n",
    "train_targets += dev_targets\n",
    "train_labels += dev_labels\n",
    "\n",
    "\n",
    "\n",
    "dev_texts,dev_targets,dev_labels = util.readACL(os.path.join(folder,'restaurants_test.txt'),binary)#Restaurants\n",
    "test_texts, test_targets, test_labels = [],[],[]\n",
    "\n",
    "def get_dep(filename):\n",
    "    fin = open(filename,'r')\n",
    "    nets = []\n",
    "    net = []\n",
    "    for line in fin:\n",
    "        if line == '\\n':\n",
    "            nets.append(net)\n",
    "            net = []\n",
    "        else:\n",
    "            e = line.replace('(','|').replace(')','|').replace(', ','|').split('|')\n",
    "            #if e[0] == 'root':\n",
    "            #    continue\n",
    "            relation = e[0]\n",
    "            src = '-'.join(e[1].split('-')[0:-1]).strip()\n",
    "            src_index = int(e[1].split('-')[-1])\n",
    "            tgt = '-'.join(e[2].split('-')[0:-1]).strip()\n",
    "            tgt_index = int(e[2].split('-')[-1])\n",
    "            net.append([src,tgt,relation,src_index,tgt_index])\n",
    "    return nets\n",
    "\n",
    "train_deps = get_dep(os.path.join(folder,'rest_train_text_mask.txt.dep'))\n",
    "\n",
    "dev_deps = get_dep(os.path.join(folder,'rest_dev_text_mask.txt.dep'))\n",
    "train_deps += dev_deps\n",
    "dev_deps = get_dep(os.path.join(folder,'rest_test_text_mask.txt.dep'))\n",
    "test_deps = []\n",
    "vocab = {'_UNKNOWN_':0}\n",
    "\n",
    "def get_target_pos(deps,targets):\n",
    "    target_pos = []\n",
    "    texts = []\n",
    "    for i,t in enumerate(targets):\n",
    "        dep = deps[i]\n",
    "        word2index = {}\n",
    "        index2word = {}\n",
    "        for edge in dep:\n",
    "            index2word[edge[3]] = edge[0]\n",
    "            index2word[edge[4]] = edge[1]\n",
    "        text = [index2word[i] for i in range(1,len(index2word))]\n",
    "        texts.append(text)\n",
    "        for w in text:\n",
    "            if w not in vocab:\n",
    "                vocab[w] = len(vocab)\n",
    "        for w in t.split():\n",
    "            if w not in vocab:\n",
    "                vocab[w] = len(vocab)\n",
    "        target_pos.append(text.index('TargetTarget'))\n",
    "        \n",
    "    return target_pos,texts\n",
    "train_pos,train_texts = get_target_pos(train_deps,train_targets)\n",
    "dev_pos,dev_texts = get_target_pos(dev_deps,dev_targets)\n",
    "test_pos,test_texts = get_target_pos(test_deps,test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(folder,'train_wordpiece_pair_'+bert)):\n",
    "    train_bert_reps = pickle.load(open(os.path.join(folder,'train_wordpiece_pair_'+os.path.basename(bert)), 'rb'))\n",
    "    dev_bert_reps = pickle.load(open(os.path.join(folder,'dev_wordpiece_pair_'+os.path.basename(bert)), 'rb'))\n",
    "    test_bert_reps = pickle.load(open(os.path.join(folder,'test_wordpiece_pair_'+os.path.basename(bert)), 'rb'))\n",
    "    vocab = pickle.load(open(os.path.join(folder,'wordpiece_pair_'+os.path.basename(bert))+'.vocab', 'rb'))\n",
    "else:\n",
    "\n",
    "    import torch\n",
    "    from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "    # OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "    import logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    # Load pre-trained model tokenizer (vocabulary)\n",
    "    tokenizer = BertTokenizer.from_pretrained(bert)\n",
    "\n",
    "    # Tokenized input\n",
    "\n",
    "\n",
    "    # Load pre-trained model (weights)\n",
    "    model = BertModel.from_pretrained(bert)\n",
    "    model.eval()\n",
    "\n",
    "    # If you have a GPU, put everything on cuda\n",
    "\n",
    "    model.to('cuda')\n",
    "\n",
    "    # Predict hidden states features for each layer\n",
    "\n",
    "    # We have a hidden states for each of the 12 layers in model bert-base-uncased\n",
    "    #assert len(encoded_layers) == 12\n",
    "    def get_bert(train_texts,train_targets):\n",
    "        train_bert_reps = []\n",
    "        for i,t in enumerate(train_texts):\n",
    "            pos = t.index('TargetTarget')\n",
    "            length = len(train_targets[i].split())\n",
    "            if pos == 0:\n",
    "                input_t = train_targets[i].split()+t[pos+1:]\n",
    "            else:\n",
    "                input_t = t[0:pos]+train_targets[i].split()+t[pos+1:]\n",
    "            word_pieces = []\n",
    "            word_map = []\n",
    "            for w in input_t:\n",
    "                word_piece = tokenizer.tokenize(w)\n",
    "                word_map.append([len(word_pieces),len(word_pieces)+len(word_piece)])\n",
    "                word_pieces += word_piece\n",
    "\n",
    "            aspect_pieces = tokenizer.tokenize(train_targets[i])\n",
    "            #aspect_tokens = tokenizer.convert_tokens_to_ids(['[CLS]']+aspect_pieces+['[SEP]'])\n",
    "            #aspect_segments = [0]*len(aspect_tokens)\n",
    "            #with torch.no_grad():\n",
    "            #    aspect_layers, _ = model(torch.tensor([aspect_tokens]).to('cuda'), torch.tensor([aspect_segments]).to('cuda'))\n",
    "            #aspect = aspect_layers[-1][0,0].to('cpu').numpy()\n",
    "            indexed_tokens = tokenizer.convert_tokens_to_ids(['[CLS]']+word_pieces+['[SEP]']+aspect_pieces+['[SEP]'])\n",
    "            segments_ids = [0]*(len(word_pieces)+2)+[1]*(len(aspect_pieces)+1)\n",
    "\n",
    "            tokens_tensor = torch.tensor([indexed_tokens])\n",
    "            segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "            tokens_tensor = tokens_tensor.to('cuda')\n",
    "            segments_tensors = segments_tensors.to('cuda')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                encoded_layers, _ = model(tokens_tensor, segments_tensors)\n",
    "            whole = encoded_layers[-1][0,1:1+len(word_pieces)].to('cpu').numpy()\n",
    "            rep = []\n",
    "            for i,w in enumerate(input_t):\n",
    "                rep.append(whole[word_map[i][0]:word_map[i][1]].mean(0))\n",
    "            whole = np.stack(rep)\n",
    "\n",
    "            if length == 1:\n",
    "                train_bert_reps.append(whole)\n",
    "            else:\n",
    "                context = [whole[0:pos],whole[pos+length:]]\n",
    "                aspect = whole[pos:pos+length]\n",
    "                train_bert_reps.append(np.concatenate([context[0],aspect.mean(0,keepdims=True),context[1]],0))\n",
    "            assert len(train_bert_reps[-1])==len(t)\n",
    "        return train_bert_reps\n",
    "\n",
    "    train_bert_reps = get_bert(train_texts,train_targets)\n",
    "    dev_bert_reps = get_bert(dev_texts,dev_targets)\n",
    "    test_bert_reps = get_bert(test_texts,test_targets)\n",
    "    pickle.dump(train_bert_reps, open(os.path.join(folder,'train_wordpiece_pair_'+os.path.basename(bert)), 'wb'))\n",
    "    pickle.dump(dev_bert_reps, open(os.path.join(folder,'dev_wordpiece_pair_'+os.path.basename(bert)), 'wb'))\n",
    "    pickle.dump(test_bert_reps, open(os.path.join(folder,'test_wordpiece_pair_'+os.path.basename(bert)), 'wb'))\n",
    "    pickle.dump(vocab, open(os.path.join(folder,'wordpiece_pair_'+os.path.basename(bert))+'.vocab', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(text,dep,pos,aspect):\n",
    "    net = []\n",
    "    for edge in dep:\n",
    "        if edge[2]=='root':\n",
    "            continue\n",
    "        net.append([edge[3]-1,edge[4]-1])\n",
    "        if bidirection:\n",
    "            net.append([edge[4]-1,edge[3]-1])\n",
    "    word_index = [vocab[w] for w in text]\n",
    "    aspect_index = [vocab[w] for w in aspect.split()]\n",
    "    if len(aspect_index)>max_aspect_length:\n",
    "        aspect_index = aspect_index[0:max_aspect_length]\n",
    "    else:\n",
    "        aspect_index = aspect_index + [0]*(max_aspect_length-len(aspect_index))\n",
    "    aspect_length = len(aspect.split())\n",
    "    target_node = pos\n",
    "    return word_index,net,target_node,aspect_index,aspect_length\n",
    "\n",
    "def get_batch(texts,deps,pos,aspects):\n",
    "    word_indices = []\n",
    "    nets = []\n",
    "    target_nodes = []\n",
    "    aspect_indices = []\n",
    "    aspect_lengths = []\n",
    "    for i in range(len(texts)):\n",
    "        word_index,net,target_node,aspect_index,aspect_length= transform(texts[i],deps[i],pos[i],aspects[i])\n",
    "        nets.append(np.array(net)+len(word_indices))\n",
    "        target_nodes.append(target_node+len(word_indices))\n",
    "        word_indices += word_index\n",
    "\n",
    "        aspect_indices += [aspect_index]\n",
    "        aspect_lengths += [aspect_length]\n",
    "\n",
    "    return word_indices, np.concatenate(nets,0),target_nodes,aspect_indices,aspect_lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Net(torch.nn.Module):\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        \\n        #self.lin1 = torch.nn.Linear(hidden_size, hidden_size)\\n        self.convs = torch.nn.ModuleList()\\n        for i in range(layer):            \\n            self.convs.append(GATConv(hidden_size, hidden_size//heads, heads=heads))\\n        self.lin3 = torch.nn.Linear(hidden_size, num_classes)\\n\\n    def forward(self, x, edge_index):\\n        if dropout>0:\\n            x = F.dropout(x,p=dropout, training=self.training)\\n        for i in range(layer):\\n            x = F.elu(self.convs[i](x, edge_index))\\n        x = self.lin3(x)\\n        return x\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def glorot(tensor):\n",
    "    stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "    if tensor is not None:\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(layer):            \n",
    "            self.convs.append(GATConv(hidden_size, hidden_size//heads, heads=heads,dropout = att_dropout))\n",
    "        self.lin3 = torch.nn.Linear(hidden_size, num_classes)\n",
    "        self.rnn = torch.nn.LSTM(hidden_size,hidden_size,1)\n",
    "        glorot(self.lin3.weight)\n",
    "        glorot(self.lin1.weight)\n",
    "    def forward(self, x, edge_index):\n",
    "        if dropout>0:\n",
    "            x = F.dropout(x,p=dropout, training=self.training)\n",
    "        x = self.lin1(x)\n",
    "        #x = self.lin1(x)        \n",
    "        output,(h,c) = self.rnn(torch.unsqueeze(x,0))\n",
    "        x = torch.squeeze(h,0)\n",
    "\n",
    "        for i in range(layer):\n",
    "            #if i == 0:\n",
    "            #    output,(h,c) = self.rnn(torch.unsqueeze(F.elu(self.convs[i](x, edge_index)),0))\n",
    "            #else:\n",
    "            output,(h,c) = self.rnn(torch.unsqueeze(F.elu(self.convs[i](x, edge_index)),0),(h,c))\n",
    "            x = torch.squeeze(h,0)\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "'''\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #self.lin1 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(layer):            \n",
    "            self.convs.append(GATConv(hidden_size, hidden_size//heads, heads=heads))\n",
    "        self.lin3 = torch.nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        if dropout>0:\n",
    "            x = F.dropout(x,p=dropout, training=self.training)\n",
    "        for i in range(layer):\n",
    "            x = F.elu(self.convs[i](x, edge_index))\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda memory allocated: 6154240\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#word_embed.to(device)\n",
    "model = Net().to(device)\n",
    "\n",
    "loss_op = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(list(model.parameters())+list(word_embed.parameters()), lr=lr)\n",
    "print(\"cuda memory allocated:\", torch.cuda.memory_allocated(device=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(train_texts,train_targets,train_labels,train_deps,train_pos,train_bert_reps):\n",
    "    model.train()\n",
    "    global total_time\n",
    "\n",
    "    if shuffle:\n",
    "        c = list(zip(train_texts,train_targets,train_labels,train_deps,train_pos,train_bert_reps))\n",
    "        np.random.shuffle(c)\n",
    "        train_texts,train_targets,train_labels,train_deps,train_pos,train_bert_reps = map(list,zip(*c))\n",
    "    total_loss = 0\n",
    "    for i in range(0,len(train_texts),batch_size):\n",
    "        s = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        word_index,net,target_node,aspect_index,aspect_length = get_batch(train_texts[i:i+batch_size],train_deps[i:i+batch_size],train_pos[i:i+batch_size],train_targets[i:i+batch_size])\n",
    "        \n",
    "        #x = torch.tensor(data=np.array(word_index),dtype=torch.long )\n",
    "\n",
    "        #x_aspect = torch.tensor(data=np.array(aspect_index),dtype=torch.long )\n",
    "        #aspect_length = torch.tensor(data=aspect_length,dtype=torch.float ).view([-1,1])\n",
    "\n",
    "        #aspect_vectors = word_embed(x_aspect.to(device)).sum(1)/aspect_length.to(device)\n",
    "        word_vectors = np.concatenate(train_bert_reps[i:i+batch_size],0)\n",
    "        word_vectors = torch.tensor(word_vectors).to(device)\n",
    "        #word_vectors = word_embed(x.to(device))\n",
    "        #word_vectors[target_node] = aspect_vectors\n",
    "        net_tensor = torch.tensor(np.array(net).transpose())\n",
    "        target_node_tensor = torch.tensor(target_node,dtype=torch.long)\n",
    "        batch_labels = np.array(train_labels[i:i+batch_size])+1\n",
    "        y_tensor = torch.tensor(batch_labels,dtype=torch.long).to(device)\n",
    "        \n",
    "\n",
    "        target_rep = model(word_vectors.to(device), net_tensor.to(device))[target_node_tensor]\n",
    "        \n",
    "        if label_smooth:\n",
    "            n_class = 3#pred.size(1)\n",
    "\n",
    "            one_hot = torch.zeros_like(target_rep).scatter(1, y_tensor.view(-1, 1), 1)\n",
    "            one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "            log_prb = F.log_softmax(target_rep, dim=1)\n",
    "            loss = -(one_hot * log_prb).sum(dim=1).mean()\n",
    "            batch_loss = loss.item()\n",
    "        else:  \n",
    "            \n",
    "            loss = loss_op(target_rep, y_tensor.to(device))\n",
    "            batch_loss = loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss +=  batch_loss*len(target_node)\n",
    "        total_time += time.time()-s\n",
    "    return total_loss/len(train_texts)\n",
    "\n",
    "\n",
    "def test(texts,targets,labels,deps,pos,bert_reps):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_acc = 0\n",
    "        for i in range(0,len(texts),batch_size):\n",
    "            word_index,net,target_node,aspect_index,aspect_length = get_batch(texts[i:i+batch_size],deps[i:i+batch_size],pos[i:i+batch_size],targets[i:i+batch_size])\n",
    "            #x = torch.tensor(data=np.array(word_index),dtype=torch.long )\n",
    "            #x_aspect = torch.tensor(data=np.array(aspect_index),dtype=torch.long )\n",
    "            #aspect_length = torch.tensor(data=aspect_length,dtype=torch.float ).view([-1,1])\n",
    "\n",
    "            #aspect_vectors = word_embed(x_aspect.to(device)).sum(1)/aspect_length.to(device)\n",
    "            \n",
    "            #word_vectors = word_embed(x.to(device))\n",
    "            #word_vectors[target_node] = aspect_vectors\n",
    "            word_vectors = np.concatenate(bert_reps[i:i+batch_size],0)\n",
    "            word_vectors = torch.tensor(word_vectors).to(device)\n",
    "            net_tensor = torch.tensor(np.array(net).transpose())\n",
    "            target_node_tensor = torch.tensor(target_node,dtype=torch.long)\n",
    "            y_tensor = torch.tensor(np.array(labels[i:i+batch_size])+1,dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                target_rep = model(word_vectors.to(device), net_tensor.to(device))[target_node_tensor]\n",
    "            pred =  target_rep.max(1)[1].cpu().numpy()\n",
    "            acc = (pred == np.array(labels[i:i+batch_size])+1)\n",
    "\n",
    "            total_acc += acc.sum()\n",
    "    return total_acc / len(texts)\n",
    "\n",
    "val_acc = []\n",
    "test_acc = [0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00, Loss: 0.8040, Acc: 0.7696, Test_Acc: 0.0000\n",
      "Epoch: 01, Loss: 0.6506, Acc: 0.7679, Test_Acc: 0.0000\n",
      "Epoch: 02, Loss: 0.6221, Acc: 0.8063, Test_Acc: 0.0000\n",
      "Epoch: 03, Loss: 0.5941, Acc: 0.7920, Test_Acc: 0.0000\n",
      "Epoch: 04, Loss: 0.5723, Acc: 0.8125, Test_Acc: 0.0000\n",
      "Epoch: 05, Loss: 0.5457, Acc: 0.8000, Test_Acc: 0.0000\n",
      "Epoch: 06, Loss: 0.5618, Acc: 0.7830, Test_Acc: 0.0000\n",
      "Epoch: 07, Loss: 0.5404, Acc: 0.7991, Test_Acc: 0.0000\n",
      "Epoch: 08, Loss: 0.5373, Acc: 0.8107, Test_Acc: 0.0000\n",
      "Epoch: 09, Loss: 0.5036, Acc: 0.8179, Test_Acc: 0.0000\n",
      "Epoch: 10, Loss: 0.5028, Acc: 0.8134, Test_Acc: 0.0000\n",
      "Epoch: 11, Loss: 0.4942, Acc: 0.8071, Test_Acc: 0.0000\n",
      "average time 1.9017212986946106\n",
      "model size 1485303\n",
      "0.8178571428571428 0\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=l2)\n",
    "#label_smooth\n",
    "import time\n",
    "total_time = 0\n",
    "#epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    loss = train(train_texts,train_targets,train_labels,train_deps,train_pos,train_bert_reps)\n",
    "    val_acc.append(test(dev_texts,dev_targets,dev_labels,dev_deps,dev_pos,dev_bert_reps))\n",
    "\n",
    "    #if len(val_acc)>tolorence and val_acc[-1]<val_acc[-tolorence]+0.0001:\n",
    "    #    break\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Acc: {:.4f}, Test_Acc: {:.4f}'.format(epoch, loss, val_acc[-1], test_acc[-1]))\n",
    "    if loss <0.5:\n",
    "        break\n",
    "print('average time',(total_time)/(epoch+1))\n",
    "print('model size',sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "'''\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr/10)\n",
    "print('change learning rate')\n",
    "for epoch in range(epochs):\n",
    "    loss = train()\n",
    "    val_acc.append(test(val_loader))\n",
    "    if val_acc[-1] >= max(val_acc):\n",
    "        test_acc.append(test(test_loader))\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Acc: {:.4f}, Test_Acc: {:.4f}'.format(epoch, loss, val_acc[-1], test_acc[-1]))\n",
    "'''\n",
    "print(max(val_acc),test_acc[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00, Loss: 0.4695, Acc: 0.8080, Test_Acc: 0.0000\n",
      "Epoch: 01, Loss: 0.4744, Acc: 0.8089, Test_Acc: 0.0000\n",
      "Epoch: 02, Loss: 0.4661, Acc: 0.8080, Test_Acc: 0.0000\n",
      "Epoch: 03, Loss: 0.4748, Acc: 0.8107, Test_Acc: 0.0000\n",
      "Epoch: 04, Loss: 0.4721, Acc: 0.8107, Test_Acc: 0.0000\n",
      "Epoch: 05, Loss: 0.4605, Acc: 0.8107, Test_Acc: 0.0000\n",
      "Epoch: 06, Loss: 0.4626, Acc: 0.8116, Test_Acc: 0.0000\n",
      "Epoch: 07, Loss: 0.4599, Acc: 0.8125, Test_Acc: 0.0000\n",
      "Epoch: 08, Loss: 0.4666, Acc: 0.8152, Test_Acc: 0.0000\n",
      "Epoch: 09, Loss: 0.4626, Acc: 0.8152, Test_Acc: 0.0000\n",
      "Epoch: 10, Loss: 0.4557, Acc: 0.8161, Test_Acc: 0.0000\n",
      "Epoch: 11, Loss: 0.4599, Acc: 0.8161, Test_Acc: 0.0000\n",
      "Epoch: 12, Loss: 0.4463, Acc: 0.8161, Test_Acc: 0.0000\n",
      "Epoch: 13, Loss: 0.4571, Acc: 0.8196, Test_Acc: 0.0000\n",
      "Epoch: 14, Loss: 0.4556, Acc: 0.8214, Test_Acc: 0.0000\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr,weight_decay=l2)\n",
    "for epoch in range(epochs):\n",
    "    loss = train(train_texts,train_targets,train_labels,train_deps,train_pos,train_bert_reps)\n",
    "    val_acc.append(test(dev_texts,dev_targets,dev_labels,dev_deps,dev_pos,dev_bert_reps))\n",
    "    \n",
    "    #if len(val_acc)>tolorence and val_acc[-1]<val_acc[-tolorence]+0.0001:\n",
    "    #    break\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Acc: {:.4f}, Test_Acc: {:.4f}'.format(epoch, loss, val_acc[-1], test_acc[-1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
