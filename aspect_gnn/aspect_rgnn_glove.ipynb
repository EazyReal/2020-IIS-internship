{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "import torch\n",
    "import util\n",
    "import random\n",
    "import pickle\n",
    "import glob\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = False\n",
    "hidden_size = 300\n",
    "label_smooth = False\n",
    "eps = 0\n",
    "embed_files = [#'/home/binxuanh/resources/embeddings/GoogleNews-vectors-negative300.txt',\n",
    "             '/home/binxuanh/resources/embeddings/glove.42B.300d.txt']\n",
    "folder = 'split_dataset/laptop_mask'\n",
    "bidirection = True\n",
    "if binary:\n",
    "    num_classes = 2\n",
    "else:\n",
    "    num_classes = 3\n",
    "dropout = 0.7\n",
    "att_dropout = 0\n",
    "lr = 0.001\n",
    "epochs = 50\n",
    "shuffle = True\n",
    "layer = 3\n",
    "heads = 6\n",
    "batch_size = 32\n",
    "embed_trainable = False\n",
    "max_aspect_length = 10\n",
    "l2 = 1e-4\n",
    "add_self = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts,train_targets,train_labels = util.readACL(os.path.join(folder,'restaurants_train.txt'),binary)\n",
    "\n",
    "dev_texts,dev_targets,dev_labels = util.readACL(os.path.join(folder,'restaurants_dev.txt'),binary)#Restaurants\n",
    "train_texts+= dev_texts\n",
    "train_targets += dev_targets\n",
    "train_labels += dev_labels\n",
    "\n",
    "\n",
    "\n",
    "dev_texts,dev_targets,dev_labels = util.readACL(os.path.join(folder,'restaurants_test.txt'),binary)#Restaurants\n",
    "test_texts, test_targets, test_labels = [],[],[]\n",
    "\n",
    "def get_dep(filename):\n",
    "    fin = open(filename,'r')\n",
    "    nets = []\n",
    "    net = []\n",
    "    for line in fin:\n",
    "        if line == '\\n':\n",
    "            nets.append(net)\n",
    "            net = []\n",
    "        else:\n",
    "            e = line.replace('(','|').replace(')','|').replace(', ','|').split('|')\n",
    "            #if e[0] == 'root':\n",
    "            #    continue\n",
    "            relation = e[0]\n",
    "            src = '-'.join(e[1].split('-')[0:-1]).strip()\n",
    "            src_index = int(e[1].split('-')[-1])\n",
    "            tgt = '-'.join(e[2].split('-')[0:-1]).strip()\n",
    "            tgt_index = int(e[2].split('-')[-1])\n",
    "            net.append([src,tgt,relation,src_index,tgt_index])\n",
    "    return nets\n",
    "\n",
    "train_deps = get_dep(os.path.join(folder,'rest_train_text_mask.txt.dep'))\n",
    "\n",
    "dev_deps = get_dep(os.path.join(folder,'rest_dev_text_mask.txt.dep'))\n",
    "train_deps += dev_deps\n",
    "dev_deps = get_dep(os.path.join(folder,'rest_test_text_mask.txt.dep'))\n",
    "test_deps = []\n",
    "vocab = {'_UNKNOWN_':0}\n",
    "\n",
    "def get_target_pos(deps,targets):\n",
    "    target_pos = []\n",
    "    texts = []\n",
    "    for i,t in enumerate(targets):\n",
    "        dep = deps[i]\n",
    "        word2index = {}\n",
    "        index2word = {}\n",
    "        for edge in dep:\n",
    "            index2word[edge[3]] = edge[0]\n",
    "            index2word[edge[4]] = edge[1]\n",
    "        text = [index2word[i] for i in range(1,len(index2word))]\n",
    "        texts.append(text)\n",
    "        for w in text:\n",
    "            if w not in vocab:\n",
    "                vocab[w] = len(vocab)\n",
    "        for w in t.split():\n",
    "            if w not in vocab:\n",
    "                vocab[w] = len(vocab)\n",
    "        target_pos.append(text.index('TargetTarget'))\n",
    "        \n",
    "    return target_pos,texts\n",
    "train_pos,train_texts = get_target_pos(train_deps,train_targets)\n",
    "dev_pos,dev_texts = get_target_pos(dev_deps,dev_targets)\n",
    "test_pos,test_texts = get_target_pos(test_deps,test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/binxuanh/resources/embeddings/glove.42B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8a84d33d5eba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0memb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/binxuanh/resources/embeddings/glove.42B.300d.txt'"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "for embed_file in embed_files:\n",
    "    base_emb = os.path.basename(embed_file)\n",
    "    if os.path.exists(os.path.join(folder,base_emb)+'.npy'):\n",
    "        emb = np.load(os.path.join(folder,base_emb)+'.npy')\n",
    "        #vocab = pickle.load(open(os.path.join(folder,base_emb)+'.vocab', 'rb'))\n",
    "    else:\n",
    "        emb = np.random.uniform(-0.01,0.01,[len(vocab), 300])\n",
    "        emb[0] = np.zeros(300)\n",
    "        with open(embed_file,'r') as fin:\n",
    "            for line in fin:\n",
    "                e = line.strip().split(' ')\n",
    "                if e[0].lower() in vocab:\n",
    "                    emb[vocab[e[0].lower()]] = np.array(e[1:],dtype=float)\n",
    "        np.save(os.path.join(folder,base_emb),emb)\n",
    "        pickle.dump(vocab, open(os.path.join(folder,base_emb+'.vocab'), 'wb'))\n",
    "    embs.append(emb)\n",
    "emb = np.concatenate(embs,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embed = torch.nn.Embedding.from_pretrained(torch.Tensor(emb))\n",
    "word_embed.weight.requires_grad=embed_trainable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(text,dep,pos,aspect):\n",
    "    net = []\n",
    "    for edge in dep:\n",
    "        if edge[2]=='root':\n",
    "            continue\n",
    "        net.append([edge[3]-1,edge[4]-1])\n",
    "        if bidirection:\n",
    "            net.append([edge[4]-1,edge[3]-1])\n",
    "    word_index = [vocab[w] for w in text]\n",
    "    aspect_index = [vocab[w] for w in aspect.split()]\n",
    "    if len(aspect_index)>max_aspect_length:\n",
    "        aspect_index = aspect_index[0:max_aspect_length]\n",
    "    else:\n",
    "        aspect_index = aspect_index + [0]*(max_aspect_length-len(aspect_index))\n",
    "    aspect_length = len(aspect.split())\n",
    "    target_node = pos\n",
    "    return word_index,net,target_node,aspect_index,aspect_length\n",
    "\n",
    "def get_batch(texts,deps,pos,aspects):\n",
    "    word_indices = []\n",
    "    nets = []\n",
    "    target_nodes = []\n",
    "    aspect_indices = []\n",
    "    aspect_lengths = []\n",
    "    for i in range(len(texts)):\n",
    "        word_index,net,target_node,aspect_index,aspect_length= transform(texts[i],deps[i],pos[i],aspects[i])\n",
    "        nets.append(np.array(net)+len(word_indices))\n",
    "        target_nodes.append(target_node+len(word_indices))\n",
    "        word_indices += word_index\n",
    "\n",
    "        aspect_indices += [aspect_index]\n",
    "        aspect_lengths += [aspect_length]\n",
    "\n",
    "    return word_indices, np.concatenate(nets,0),target_nodes,aspect_indices,aspect_lengths\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Net(torch.nn.Module):\\n    def __init__(self):\\n        super(Net, self).__init__()\\n        \\n        #self.lin1 = torch.nn.Linear(hidden_size, hidden_size)\\n        self.convs = torch.nn.ModuleList()\\n        for i in range(layer):            \\n            self.convs.append(GATConv(hidden_size, hidden_size//heads, heads=heads))\\n        self.lin3 = torch.nn.Linear(hidden_size, num_classes)\\n\\n    def forward(self, x, edge_index):\\n        if dropout>0:\\n            x = F.dropout(x,p=dropout, training=self.training)\\n        for i in range(layer):\\n            x = F.elu(self.convs[i](x, edge_index))\\n        x = self.lin3(x)\\n        return x\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def glorot(tensor):\n",
    "    stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "    if tensor is not None:\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #self.lin1 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(layer):            \n",
    "            self.convs.append(GATConv(hidden_size, hidden_size//heads, heads=heads,dropout = att_dropout))\n",
    "        self.lin3 = torch.nn.Linear(hidden_size, num_classes)\n",
    "        self.rnn = torch.nn.LSTM(hidden_size,hidden_size,1)\n",
    "        glorot(self.lin3.weight)\n",
    "    def forward(self, x, edge_index):\n",
    "        if dropout>0:\n",
    "            x = F.dropout(x,p=dropout, training=self.training)\n",
    "        #x = self.lin1(x)        \n",
    "        output,(h,c) = self.rnn(torch.unsqueeze(x,0))\n",
    "        x = torch.squeeze(h,0)\n",
    "\n",
    "        for i in range(layer):\n",
    "            #if i == 0:\n",
    "            #    output,(h,c) = self.rnn(torch.unsqueeze(F.elu(self.convs[i](x, edge_index)),0))\n",
    "            #else:\n",
    "            output,(h,c) = self.rnn(torch.unsqueeze(F.elu(self.convs[i](x, edge_index)),0),(h,c))\n",
    "            x = torch.squeeze(h,0)\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "'''\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #self.lin1 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for i in range(layer):            \n",
    "            self.convs.append(GATConv(hidden_size, hidden_size//heads, heads=heads))\n",
    "        self.lin3 = torch.nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        if dropout>0:\n",
    "            x = F.dropout(x,p=dropout, training=self.training)\n",
    "        for i in range(layer):\n",
    "            x = F.elu(self.convs[i](x, edge_index))\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "word_embed.to(device)\n",
    "model = Net().to(device)\n",
    "\n",
    "loss_op = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.Adam(list(model.parameters())+list(word_embed.parameters()), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(train_texts,train_targets,train_labels,train_deps,train_pos):\n",
    "    model.train()\n",
    "    if shuffle:\n",
    "        c = list(zip(train_texts,train_targets,train_labels,train_deps,train_pos))\n",
    "        np.random.shuffle(c)\n",
    "        train_texts,train_targets,train_labels,train_deps,train_pos = map(list,zip(*c))\n",
    "    total_loss = 0\n",
    "    for i in range(0,len(train_texts),batch_size):\n",
    "\n",
    "        word_index,net,target_node,aspect_index,aspect_length = get_batch(train_texts[i:i+batch_size],train_deps[i:i+batch_size],train_pos[i:i+batch_size],train_targets[i:i+batch_size])\n",
    "\n",
    "        x = torch.tensor(data=np.array(word_index),dtype=torch.long )\n",
    "\n",
    "        x_aspect = torch.tensor(data=np.array(aspect_index),dtype=torch.long )\n",
    "        aspect_length = torch.tensor(data=aspect_length,dtype=torch.float ).view([-1,1])\n",
    "\n",
    "        aspect_vectors = word_embed(x_aspect.to(device)).sum(1)/aspect_length.to(device)\n",
    "        \n",
    "        word_vectors = word_embed(x.to(device))\n",
    "        word_vectors[target_node] = aspect_vectors\n",
    "        net_tensor = torch.tensor(np.array(net).transpose())\n",
    "        target_node_tensor = torch.tensor(target_node,dtype=torch.long)\n",
    "        y_tensor = torch.tensor(np.array(train_labels[i:i+batch_size])+1,dtype=torch.long).to(device)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        target_rep = model(word_vectors.to(device), net_tensor.to(device))[target_node_tensor]\n",
    "        if label_smooth:\n",
    "            n_class = 3#pred.size(1)\n",
    "\n",
    "            one_hot = torch.zeros_like(target_rep).scatter(1, y_tensor.view(-1, 1), 1)\n",
    "            one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "            log_prb = F.log_softmax(target_rep, dim=1)\n",
    "            loss = -(one_hot * log_prb).sum(dim=1).mean()\n",
    "            batch_loss = loss.item()\n",
    "        else:  \n",
    "            \n",
    "            loss = loss_op(target_rep, y_tensor.to(device))\n",
    "            batch_loss = loss.item()\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss +=  batch_loss*len(target_node)\n",
    "\n",
    "    return total_loss/len(train_texts)\n",
    "\n",
    "\n",
    "def test(texts,targets,labels,deps,pos):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_acc = 0\n",
    "        for i in range(0,len(texts),batch_size):\n",
    "            word_index,net,target_node,aspect_index,aspect_length = get_batch(texts[i:i+batch_size],deps[i:i+batch_size],pos[i:i+batch_size],targets[i:i+batch_size])\n",
    "            x = torch.tensor(data=np.array(word_index),dtype=torch.long )\n",
    "            x_aspect = torch.tensor(data=np.array(aspect_index),dtype=torch.long )\n",
    "            aspect_length = torch.tensor(data=aspect_length,dtype=torch.float ).view([-1,1])\n",
    "\n",
    "            aspect_vectors = word_embed(x_aspect.to(device)).sum(1)/aspect_length.to(device)\n",
    "            \n",
    "            word_vectors = word_embed(x.to(device))\n",
    "            word_vectors[target_node] = aspect_vectors\n",
    "            net_tensor = torch.tensor(np.array(net).transpose())\n",
    "            target_node_tensor = torch.tensor(target_node,dtype=torch.long)\n",
    "            y_tensor = torch.tensor(np.array(labels[i:i+batch_size])+1,dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                target_rep = model(word_vectors.to(device), net_tensor.to(device))[target_node_tensor]\n",
    "            pred =  target_rep.max(1)[1].cpu().numpy()\n",
    "            acc = (pred == np.array(labels[i:i+batch_size])+1)\n",
    "\n",
    "            total_acc += acc.sum()\n",
    "    return total_acc / len(texts)\n",
    "\n",
    "val_acc = []\n",
    "test_acc = [0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00, Loss: 0.9974, Acc: 0.5486, Test_Acc: 0.0000\n",
      "Epoch: 01, Loss: 0.8672, Acc: 0.6285, Test_Acc: 0.0000\n",
      "Epoch: 02, Loss: 0.8036, Acc: 0.6505, Test_Acc: 0.0000\n",
      "Epoch: 03, Loss: 0.7973, Acc: 0.6991, Test_Acc: 0.0000\n",
      "Epoch: 04, Loss: 0.7597, Acc: 0.7116, Test_Acc: 0.0000\n",
      "Epoch: 05, Loss: 0.7421, Acc: 0.6959, Test_Acc: 0.0000\n",
      "Epoch: 06, Loss: 0.7135, Acc: 0.7038, Test_Acc: 0.0000\n",
      "Epoch: 07, Loss: 0.7120, Acc: 0.6834, Test_Acc: 0.0000\n",
      "Epoch: 08, Loss: 0.6906, Acc: 0.7210, Test_Acc: 0.0000\n",
      "Epoch: 09, Loss: 0.6859, Acc: 0.7351, Test_Acc: 0.0000\n",
      "Epoch: 10, Loss: 0.6690, Acc: 0.7038, Test_Acc: 0.0000\n",
      "Epoch: 11, Loss: 0.6454, Acc: 0.6991, Test_Acc: 0.0000\n",
      "Epoch: 12, Loss: 0.6347, Acc: 0.7085, Test_Acc: 0.0000\n",
      "Epoch: 13, Loss: 0.6391, Acc: 0.7226, Test_Acc: 0.0000\n",
      "Epoch: 14, Loss: 0.6296, Acc: 0.7116, Test_Acc: 0.0000\n",
      "Epoch: 15, Loss: 0.6299, Acc: 0.7476, Test_Acc: 0.0000\n",
      "Epoch: 16, Loss: 0.6040, Acc: 0.7069, Test_Acc: 0.0000\n",
      "Epoch: 17, Loss: 0.5930, Acc: 0.7257, Test_Acc: 0.0000\n",
      "Epoch: 18, Loss: 0.5691, Acc: 0.7367, Test_Acc: 0.0000\n",
      "Epoch: 19, Loss: 0.5794, Acc: 0.7382, Test_Acc: 0.0000\n",
      "Epoch: 20, Loss: 0.5532, Acc: 0.7351, Test_Acc: 0.0000\n",
      "Epoch: 21, Loss: 0.5653, Acc: 0.7414, Test_Acc: 0.0000\n",
      "Epoch: 22, Loss: 0.5403, Acc: 0.7304, Test_Acc: 0.0000\n",
      "Epoch: 23, Loss: 0.5401, Acc: 0.7100, Test_Acc: 0.0000\n",
      "Epoch: 24, Loss: 0.5104, Acc: 0.7304, Test_Acc: 0.0000\n",
      "Epoch: 25, Loss: 0.5204, Acc: 0.7335, Test_Acc: 0.0000\n",
      "Epoch: 26, Loss: 0.5075, Acc: 0.7226, Test_Acc: 0.0000\n",
      "Epoch: 27, Loss: 0.5094, Acc: 0.7147, Test_Acc: 0.0000\n",
      "Epoch: 28, Loss: 0.5062, Acc: 0.6897, Test_Acc: 0.0000\n",
      "Epoch: 29, Loss: 0.4702, Acc: 0.7288, Test_Acc: 0.0000\n",
      "0.7476489028213166 0\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=l2)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = train(train_texts,train_targets,train_labels,train_deps,train_pos)\n",
    "    val_acc.append(test(dev_texts,dev_targets,dev_labels,dev_deps,dev_pos))\n",
    "\n",
    "    #if len(val_acc)>tolorence and val_acc[-1]<val_acc[-tolorence]+0.0001:\n",
    "    #    break\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Acc: {:.4f}, Test_Acc: {:.4f}'.format(epoch, loss, val_acc[-1], test_acc[-1]))\n",
    "    if loss<0.5:\n",
    "        break\n",
    "'''\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr/10)\n",
    "print('change learning rate')\n",
    "for epoch in range(epochs):\n",
    "    loss = train()\n",
    "    val_acc.append(test(val_loader))\n",
    "    if val_acc[-1] >= max(val_acc):\n",
    "        test_acc.append(test(test_loader))\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Acc: {:.4f}, Test_Acc: {:.4f}'.format(epoch, loss, val_acc[-1], test_acc[-1]))\n",
    "'''\n",
    "print(max(val_acc),test_acc[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00, Loss: 0.4666, Acc: 0.7288, Test_Acc: 0.0000\n",
      "Epoch: 01, Loss: 0.4514, Acc: 0.7241, Test_Acc: 0.0000\n",
      "Epoch: 02, Loss: 0.4650, Acc: 0.7210, Test_Acc: 0.0000\n",
      "Epoch: 03, Loss: 0.4520, Acc: 0.7226, Test_Acc: 0.0000\n",
      "Epoch: 04, Loss: 0.4482, Acc: 0.7241, Test_Acc: 0.0000\n",
      "Epoch: 05, Loss: 0.4439, Acc: 0.7257, Test_Acc: 0.0000\n",
      "Epoch: 06, Loss: 0.4537, Acc: 0.7273, Test_Acc: 0.0000\n",
      "Epoch: 07, Loss: 0.4487, Acc: 0.7226, Test_Acc: 0.0000\n",
      "Epoch: 08, Loss: 0.4384, Acc: 0.7226, Test_Acc: 0.0000\n",
      "Epoch: 09, Loss: 0.4327, Acc: 0.7241, Test_Acc: 0.0000\n",
      "Epoch: 10, Loss: 0.4417, Acc: 0.7257, Test_Acc: 0.0000\n",
      "Epoch: 11, Loss: 0.4334, Acc: 0.7257, Test_Acc: 0.0000\n",
      "Epoch: 12, Loss: 0.4321, Acc: 0.7273, Test_Acc: 0.0000\n",
      "Epoch: 13, Loss: 0.4268, Acc: 0.7273, Test_Acc: 0.0000\n",
      "Epoch: 14, Loss: 0.4337, Acc: 0.7273, Test_Acc: 0.0000\n",
      "Epoch: 15, Loss: 0.4405, Acc: 0.7257, Test_Acc: 0.0000\n",
      "Epoch: 16, Loss: 0.4472, Acc: 0.7257, Test_Acc: 0.0000\n",
      "Epoch: 17, Loss: 0.4223, Acc: 0.7241, Test_Acc: 0.0000\n",
      "Epoch: 18, Loss: 0.4518, Acc: 0.7226, Test_Acc: 0.0000\n",
      "Epoch: 19, Loss: 0.4504, Acc: 0.7241, Test_Acc: 0.0000\n",
      "Epoch: 20, Loss: 0.4417, Acc: 0.7241, Test_Acc: 0.0000\n",
      "Epoch: 21, Loss: 0.4446, Acc: 0.7241, Test_Acc: 0.0000\n",
      "Epoch: 22, Loss: 0.4371, Acc: 0.7273, Test_Acc: 0.0000\n",
      "Epoch: 23, Loss: 0.4386, Acc: 0.7241, Test_Acc: 0.0000\n",
      "Epoch: 24, Loss: 0.4181, Acc: 0.7257, Test_Acc: 0.0000\n",
      "Epoch: 25, Loss: 0.4363, Acc: 0.7273, Test_Acc: 0.0000\n",
      "Epoch: 26, Loss: 0.4260, Acc: 0.7241, Test_Acc: 0.0000\n",
      "Epoch: 27, Loss: 0.4406, Acc: 0.7288, Test_Acc: 0.0000\n",
      "Epoch: 28, Loss: 0.4338, Acc: 0.7288, Test_Acc: 0.0000\n",
      "Epoch: 29, Loss: 0.4268, Acc: 0.7320, Test_Acc: 0.0000\n",
      "Epoch: 30, Loss: 0.4314, Acc: 0.7288, Test_Acc: 0.0000\n",
      "Epoch: 31, Loss: 0.4528, Acc: 0.7288, Test_Acc: 0.0000\n",
      "Epoch: 32, Loss: 0.4486, Acc: 0.7288, Test_Acc: 0.0000\n",
      "Epoch: 33, Loss: 0.4320, Acc: 0.7320, Test_Acc: 0.0000\n",
      "Epoch: 34, Loss: 0.4289, Acc: 0.7320, Test_Acc: 0.0000\n",
      "Epoch: 35, Loss: 0.4090, Acc: 0.7320, Test_Acc: 0.0000\n",
      "Epoch: 36, Loss: 0.4309, Acc: 0.7320, Test_Acc: 0.0000\n",
      "Epoch: 37, Loss: 0.4159, Acc: 0.7320, Test_Acc: 0.0000\n",
      "Epoch: 38, Loss: 0.4426, Acc: 0.7304, Test_Acc: 0.0000\n",
      "Epoch: 39, Loss: 0.4172, Acc: 0.7335, Test_Acc: 0.0000\n",
      "Epoch: 40, Loss: 0.4297, Acc: 0.7335, Test_Acc: 0.0000\n",
      "Epoch: 41, Loss: 0.4273, Acc: 0.7335, Test_Acc: 0.0000\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr,weight_decay=l2)\n",
    "for epoch in range(epochs):\n",
    "    loss = train(train_texts,train_targets,train_labels,train_deps,train_pos)\n",
    "    val_acc.append(test(dev_texts,dev_targets,dev_labels,dev_deps,dev_pos))\n",
    "    \n",
    "    #if len(val_acc)>tolorence and val_acc[-1]<val_acc[-tolorence]+0.0001:\n",
    "    #    break\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}, Acc: {:.4f}, Test_Acc: {:.4f}'.format(epoch, loss, val_acc[-1], test_acc[-1]))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
