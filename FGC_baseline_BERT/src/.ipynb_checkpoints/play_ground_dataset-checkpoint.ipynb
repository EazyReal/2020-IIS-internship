{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPENDENCY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './FGC_release_1.7.13/'\n",
    "data_file = data_dir + 'FGC_release_all_dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0706 07:41:06.259315 140573014021952 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /root/.cache/torch/transformers/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "247"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(data_file) as fo:\n",
    "  develop_set = json.load(fo)\n",
    "\n",
    "print(type(develop_set[0][\"QUESTIONS\"][0]))\n",
    "len(develop_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'苏东坡在中国历史上，是哪一个朝代的人？'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "develop_set[0][\"QUESTIONS\"][0][\"QTEXT_CN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['DID', 'QUESTIONS', 'DTEXT', 'DTEXT_CN', 'SENTS'])\n"
     ]
    }
   ],
   "source": [
    "print(develop_set[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['QID', 'QTYPE', 'ATYPE_', 'AMODE_', 'QTEXT', 'QTEXT_CN', 'SENTS', 'SHINT_', 'ANSWER', 'ASPAN', 'AMODE', 'ATYPE', 'AHINT', 'SHINT'])\n"
     ]
    }
   ],
   "source": [
    "print(develop_set[0][\"QUESTIONS\"][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'苏轼（1037年1月8日－1101年8月24日），'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "develop_set[0][\"SENTS\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['苏',\n",
       " '轼',\n",
       " '（',\n",
       " '103',\n",
       " '##7',\n",
       " '年',\n",
       " '1',\n",
       " '月',\n",
       " '8',\n",
       " '日',\n",
       " '－',\n",
       " '110',\n",
       " '##1',\n",
       " '年',\n",
       " '8',\n",
       " '月',\n",
       " '24',\n",
       " '日',\n",
       " '）',\n",
       " '，']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(develop_set[0][\"SENTS\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSER:\n",
    "    def __init__(self, model_name):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGC_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "        FGC release all dev.json\n",
    "        usage FGC_Dataset(file_path, mode, tokenizer)\n",
    "        for tokenizer:\n",
    "            PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "            tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "        for file_path:\n",
    "            something like ./FGC_release_1.7.13/FGC_release_all_dev.json\n",
    "        for mode:\n",
    "            [\"train\", \"develop\", \"test\"]\n",
    "    \"\"\"\n",
    "    # read, preprocessing\n",
    "    def __init__(self, data_file_ref, mode, tokenizer):\n",
    "        # load raw json\n",
    "        assert mode in [\"train\", \"develop\", \"test\"]\n",
    "        self.mode = mode\n",
    "        with open(data_file_ref) as fo:\n",
    "            self.raw_data = json.load(fo)\n",
    "        self.tokenizer = tokenizer \n",
    "        self.tokenlized_pair = None\n",
    "        \n",
    "        # generate raw pairs of q sent s\n",
    "        self.raw_pair = list()\n",
    "        for instance in self.raw_data:\n",
    "            q = instance[\"QUESTIONS\"][0][\"QTEXT_CN\"]\n",
    "            sentences = instance[\"SENTS\"]\n",
    "            for idx, sent in enumerate(sentences):\n",
    "                # check if is supporting evidence\n",
    "                lab = idx in instance[\"QUESTIONS\"][0][\"SHINT_\"]\n",
    "                self.raw_pair.append((q, sent[\"text\"], lab))\n",
    "        \n",
    "        # generate tensors \n",
    "        self.dat = list()\n",
    "        for instance in self.raw_pair:\n",
    "            q, sent, label = instance\n",
    "            \n",
    "            if mode is not \"test\":\n",
    "                label_tensor = torch.tensor(label)\n",
    "            else:\n",
    "                label_tensor = None\n",
    "            \n",
    "            # first sentence, use bert tokenizer to cut subwords\n",
    "            subwords = [\"[CLS]\"]\n",
    "            q_tokens = self.tokenizer.tokenize(q)\n",
    "            subwords.extend(q_tokens)\n",
    "            subwords.append(\"[SEP]\")\n",
    "            len_q = len(subwords)\n",
    "            \n",
    "            # second sentence\n",
    "            sent_tokens = self.tokenizer.tokenize(sent)\n",
    "            subwords.extend(sent_tokens)\n",
    "            subwords.append(\"[SEP]\")\n",
    "            len_sent = len(subwords)\n",
    "            \n",
    "            # subwords to ids, ids to torch tensor\n",
    "            ids = self.tokenizer.convert_tokens_to_ids(subwords)\n",
    "            tokens_tensor = torch.tensor(ids)\n",
    "            \n",
    "            # segments_tensor\n",
    "            segments_tensor = torch.tensor([0] * len_q + [1] * len_sent, dtype=torch.long)\n",
    "            self.dat.append((tokens_tensor, segments_tensor, label_tensor))\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    # get one data of index idx\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dat[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgc_ds = FGC_Dataset(data_file, \"develop\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '苏',\n",
       " '东',\n",
       " '坡',\n",
       " '在',\n",
       " '中',\n",
       " '国',\n",
       " '历',\n",
       " '史',\n",
       " '上',\n",
       " '，',\n",
       " '是',\n",
       " '哪',\n",
       " '一',\n",
       " '个',\n",
       " '朝',\n",
       " '代',\n",
       " '的',\n",
       " '人',\n",
       " '？',\n",
       " '[SEP]',\n",
       " '苏',\n",
       " '轼',\n",
       " '（',\n",
       " '103',\n",
       " '##7',\n",
       " '年',\n",
       " '1',\n",
       " '月',\n",
       " '8',\n",
       " '日',\n",
       " '－',\n",
       " '110',\n",
       " '##1',\n",
       " '年',\n",
       " '8',\n",
       " '月',\n",
       " '24',\n",
       " '日',\n",
       " '）',\n",
       " '，',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(fgc_ds[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
